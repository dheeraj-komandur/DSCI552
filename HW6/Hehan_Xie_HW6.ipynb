{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised, Semi-Supervised, and Unsupervised Learning\n",
    "#### (a) Use the first 20% of the positive and negative classes in the file as the test set and the rest as the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-20T01:31:43.827508Z",
     "start_time": "2019-11-20T01:31:43.824954Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-20T01:31:43.880034Z",
     "start_time": "2019-11-20T01:31:43.832478Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.30010</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>...</td>\n",
       "      <td>25.380</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.16220</td>\n",
       "      <td>0.66560</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.08690</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>...</td>\n",
       "      <td>24.990</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.12380</td>\n",
       "      <td>0.18660</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.19740</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>...</td>\n",
       "      <td>23.570</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.14440</td>\n",
       "      <td>0.42450</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.24140</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>...</td>\n",
       "      <td>14.910</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.20980</td>\n",
       "      <td>0.86630</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.19800</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>...</td>\n",
       "      <td>22.540</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.13740</td>\n",
       "      <td>0.20500</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>564</td>\n",
       "      <td>1</td>\n",
       "      <td>21.56</td>\n",
       "      <td>22.39</td>\n",
       "      <td>142.00</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>0.11100</td>\n",
       "      <td>0.11590</td>\n",
       "      <td>0.24390</td>\n",
       "      <td>0.13890</td>\n",
       "      <td>0.1726</td>\n",
       "      <td>...</td>\n",
       "      <td>25.450</td>\n",
       "      <td>26.40</td>\n",
       "      <td>166.10</td>\n",
       "      <td>2027.0</td>\n",
       "      <td>0.14100</td>\n",
       "      <td>0.21130</td>\n",
       "      <td>0.4107</td>\n",
       "      <td>0.2216</td>\n",
       "      <td>0.2060</td>\n",
       "      <td>0.07115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>565</td>\n",
       "      <td>1</td>\n",
       "      <td>20.13</td>\n",
       "      <td>28.25</td>\n",
       "      <td>131.20</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.09780</td>\n",
       "      <td>0.10340</td>\n",
       "      <td>0.14400</td>\n",
       "      <td>0.09791</td>\n",
       "      <td>0.1752</td>\n",
       "      <td>...</td>\n",
       "      <td>23.690</td>\n",
       "      <td>38.25</td>\n",
       "      <td>155.00</td>\n",
       "      <td>1731.0</td>\n",
       "      <td>0.11660</td>\n",
       "      <td>0.19220</td>\n",
       "      <td>0.3215</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>0.06637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>566</td>\n",
       "      <td>1</td>\n",
       "      <td>16.60</td>\n",
       "      <td>28.08</td>\n",
       "      <td>108.30</td>\n",
       "      <td>858.1</td>\n",
       "      <td>0.08455</td>\n",
       "      <td>0.10230</td>\n",
       "      <td>0.09251</td>\n",
       "      <td>0.05302</td>\n",
       "      <td>0.1590</td>\n",
       "      <td>...</td>\n",
       "      <td>18.980</td>\n",
       "      <td>34.12</td>\n",
       "      <td>126.70</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.30940</td>\n",
       "      <td>0.3403</td>\n",
       "      <td>0.1418</td>\n",
       "      <td>0.2218</td>\n",
       "      <td>0.07820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>567</td>\n",
       "      <td>1</td>\n",
       "      <td>20.60</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.10</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>0.11780</td>\n",
       "      <td>0.27700</td>\n",
       "      <td>0.35140</td>\n",
       "      <td>0.15200</td>\n",
       "      <td>0.2397</td>\n",
       "      <td>...</td>\n",
       "      <td>25.740</td>\n",
       "      <td>39.42</td>\n",
       "      <td>184.60</td>\n",
       "      <td>1821.0</td>\n",
       "      <td>0.16500</td>\n",
       "      <td>0.86810</td>\n",
       "      <td>0.9387</td>\n",
       "      <td>0.2650</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.12400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>568</td>\n",
       "      <td>0</td>\n",
       "      <td>7.76</td>\n",
       "      <td>24.54</td>\n",
       "      <td>47.92</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.05263</td>\n",
       "      <td>0.04362</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.1587</td>\n",
       "      <td>...</td>\n",
       "      <td>9.456</td>\n",
       "      <td>30.37</td>\n",
       "      <td>59.16</td>\n",
       "      <td>268.6</td>\n",
       "      <td>0.08996</td>\n",
       "      <td>0.06444</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>0.07039</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     1      2      3       4       5        6        7        8        9   \\\n",
       "0     1  17.99  10.38  122.80  1001.0  0.11840  0.27760  0.30010  0.14710   \n",
       "1     1  20.57  17.77  132.90  1326.0  0.08474  0.07864  0.08690  0.07017   \n",
       "2     1  19.69  21.25  130.00  1203.0  0.10960  0.15990  0.19740  0.12790   \n",
       "3     1  11.42  20.38   77.58   386.1  0.14250  0.28390  0.24140  0.10520   \n",
       "4     1  20.29  14.34  135.10  1297.0  0.10030  0.13280  0.19800  0.10430   \n",
       "..   ..    ...    ...     ...     ...      ...      ...      ...      ...   \n",
       "564   1  21.56  22.39  142.00  1479.0  0.11100  0.11590  0.24390  0.13890   \n",
       "565   1  20.13  28.25  131.20  1261.0  0.09780  0.10340  0.14400  0.09791   \n",
       "566   1  16.60  28.08  108.30   858.1  0.08455  0.10230  0.09251  0.05302   \n",
       "567   1  20.60  29.33  140.10  1265.0  0.11780  0.27700  0.35140  0.15200   \n",
       "568   0   7.76  24.54   47.92   181.0  0.05263  0.04362  0.00000  0.00000   \n",
       "\n",
       "         10  ...      22     23      24      25       26       27      28  \\\n",
       "0    0.2419  ...  25.380  17.33  184.60  2019.0  0.16220  0.66560  0.7119   \n",
       "1    0.1812  ...  24.990  23.41  158.80  1956.0  0.12380  0.18660  0.2416   \n",
       "2    0.2069  ...  23.570  25.53  152.50  1709.0  0.14440  0.42450  0.4504   \n",
       "3    0.2597  ...  14.910  26.50   98.87   567.7  0.20980  0.86630  0.6869   \n",
       "4    0.1809  ...  22.540  16.67  152.20  1575.0  0.13740  0.20500  0.4000   \n",
       "..      ...  ...     ...    ...     ...     ...      ...      ...     ...   \n",
       "564  0.1726  ...  25.450  26.40  166.10  2027.0  0.14100  0.21130  0.4107   \n",
       "565  0.1752  ...  23.690  38.25  155.00  1731.0  0.11660  0.19220  0.3215   \n",
       "566  0.1590  ...  18.980  34.12  126.70  1124.0  0.11390  0.30940  0.3403   \n",
       "567  0.2397  ...  25.740  39.42  184.60  1821.0  0.16500  0.86810  0.9387   \n",
       "568  0.1587  ...   9.456  30.37   59.16   268.6  0.08996  0.06444  0.0000   \n",
       "\n",
       "         29      30       31  \n",
       "0    0.2654  0.4601  0.11890  \n",
       "1    0.1860  0.2750  0.08902  \n",
       "2    0.2430  0.3613  0.08758  \n",
       "3    0.2575  0.6638  0.17300  \n",
       "4    0.1625  0.2364  0.07678  \n",
       "..      ...     ...      ...  \n",
       "564  0.2216  0.2060  0.07115  \n",
       "565  0.1628  0.2572  0.06637  \n",
       "566  0.1418  0.2218  0.07820  \n",
       "567  0.2650  0.4087  0.12400  \n",
       "568  0.0000  0.2871  0.07039  \n",
       "\n",
       "[569 rows x 31 columns]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('wdbc.data', header=None)\n",
    "dataset = dataset.iloc[:, 1:]\n",
    "dataset.replace('M', 1, inplace=True)\n",
    "dataset.replace('B', 0, inplace=True)\n",
    "\n",
    "positiveDataset = dataset[dataset[1]==1]\n",
    "negativeeDataset = dataset[dataset[1]==0]\n",
    "dataset\n",
    "# positiveDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-20T01:31:43.914107Z",
     "start_time": "2019-11-20T01:31:43.882251Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>...</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>121</td>\n",
       "      <td>18.66</td>\n",
       "      <td>17.12</td>\n",
       "      <td>121.40</td>\n",
       "      <td>1077.0</td>\n",
       "      <td>0.10540</td>\n",
       "      <td>0.11000</td>\n",
       "      <td>0.14570</td>\n",
       "      <td>0.08665</td>\n",
       "      <td>0.1966</td>\n",
       "      <td>0.06213</td>\n",
       "      <td>...</td>\n",
       "      <td>22.250</td>\n",
       "      <td>24.90</td>\n",
       "      <td>145.40</td>\n",
       "      <td>1549.0</td>\n",
       "      <td>0.15030</td>\n",
       "      <td>0.22910</td>\n",
       "      <td>0.3272</td>\n",
       "      <td>0.16740</td>\n",
       "      <td>0.2894</td>\n",
       "      <td>0.08456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122</td>\n",
       "      <td>24.25</td>\n",
       "      <td>20.20</td>\n",
       "      <td>166.20</td>\n",
       "      <td>1761.0</td>\n",
       "      <td>0.14470</td>\n",
       "      <td>0.28670</td>\n",
       "      <td>0.42680</td>\n",
       "      <td>0.20120</td>\n",
       "      <td>0.2655</td>\n",
       "      <td>0.06877</td>\n",
       "      <td>...</td>\n",
       "      <td>26.020</td>\n",
       "      <td>23.99</td>\n",
       "      <td>180.90</td>\n",
       "      <td>2073.0</td>\n",
       "      <td>0.16960</td>\n",
       "      <td>0.42440</td>\n",
       "      <td>0.5803</td>\n",
       "      <td>0.22480</td>\n",
       "      <td>0.3222</td>\n",
       "      <td>0.08009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126</td>\n",
       "      <td>13.61</td>\n",
       "      <td>24.69</td>\n",
       "      <td>87.76</td>\n",
       "      <td>572.6</td>\n",
       "      <td>0.09258</td>\n",
       "      <td>0.07862</td>\n",
       "      <td>0.05285</td>\n",
       "      <td>0.03085</td>\n",
       "      <td>0.1761</td>\n",
       "      <td>0.06130</td>\n",
       "      <td>...</td>\n",
       "      <td>16.890</td>\n",
       "      <td>35.64</td>\n",
       "      <td>113.20</td>\n",
       "      <td>848.7</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.28840</td>\n",
       "      <td>0.3796</td>\n",
       "      <td>0.13290</td>\n",
       "      <td>0.3470</td>\n",
       "      <td>0.07900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>127</td>\n",
       "      <td>19.00</td>\n",
       "      <td>18.91</td>\n",
       "      <td>123.40</td>\n",
       "      <td>1138.0</td>\n",
       "      <td>0.08217</td>\n",
       "      <td>0.08028</td>\n",
       "      <td>0.09271</td>\n",
       "      <td>0.05627</td>\n",
       "      <td>0.1946</td>\n",
       "      <td>0.05044</td>\n",
       "      <td>...</td>\n",
       "      <td>22.320</td>\n",
       "      <td>25.73</td>\n",
       "      <td>148.20</td>\n",
       "      <td>1538.0</td>\n",
       "      <td>0.10210</td>\n",
       "      <td>0.22640</td>\n",
       "      <td>0.3207</td>\n",
       "      <td>0.12180</td>\n",
       "      <td>0.2841</td>\n",
       "      <td>0.06541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>129</td>\n",
       "      <td>19.79</td>\n",
       "      <td>25.12</td>\n",
       "      <td>130.40</td>\n",
       "      <td>1192.0</td>\n",
       "      <td>0.10150</td>\n",
       "      <td>0.15890</td>\n",
       "      <td>0.25450</td>\n",
       "      <td>0.11490</td>\n",
       "      <td>0.2202</td>\n",
       "      <td>0.06113</td>\n",
       "      <td>...</td>\n",
       "      <td>22.630</td>\n",
       "      <td>33.58</td>\n",
       "      <td>148.70</td>\n",
       "      <td>1589.0</td>\n",
       "      <td>0.12750</td>\n",
       "      <td>0.38610</td>\n",
       "      <td>0.5673</td>\n",
       "      <td>0.17320</td>\n",
       "      <td>0.3305</td>\n",
       "      <td>0.08465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>558</td>\n",
       "      <td>14.59</td>\n",
       "      <td>22.68</td>\n",
       "      <td>96.39</td>\n",
       "      <td>657.1</td>\n",
       "      <td>0.08473</td>\n",
       "      <td>0.13300</td>\n",
       "      <td>0.10290</td>\n",
       "      <td>0.03736</td>\n",
       "      <td>0.1454</td>\n",
       "      <td>0.06147</td>\n",
       "      <td>...</td>\n",
       "      <td>15.480</td>\n",
       "      <td>27.27</td>\n",
       "      <td>105.90</td>\n",
       "      <td>733.5</td>\n",
       "      <td>0.10260</td>\n",
       "      <td>0.31710</td>\n",
       "      <td>0.3662</td>\n",
       "      <td>0.11050</td>\n",
       "      <td>0.2258</td>\n",
       "      <td>0.08004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>559</td>\n",
       "      <td>11.51</td>\n",
       "      <td>23.93</td>\n",
       "      <td>74.52</td>\n",
       "      <td>403.5</td>\n",
       "      <td>0.09261</td>\n",
       "      <td>0.10210</td>\n",
       "      <td>0.11120</td>\n",
       "      <td>0.04105</td>\n",
       "      <td>0.1388</td>\n",
       "      <td>0.06570</td>\n",
       "      <td>...</td>\n",
       "      <td>12.480</td>\n",
       "      <td>37.16</td>\n",
       "      <td>82.28</td>\n",
       "      <td>474.2</td>\n",
       "      <td>0.12980</td>\n",
       "      <td>0.25170</td>\n",
       "      <td>0.3630</td>\n",
       "      <td>0.09653</td>\n",
       "      <td>0.2112</td>\n",
       "      <td>0.08732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>14.05</td>\n",
       "      <td>27.15</td>\n",
       "      <td>91.38</td>\n",
       "      <td>600.4</td>\n",
       "      <td>0.09929</td>\n",
       "      <td>0.11260</td>\n",
       "      <td>0.04462</td>\n",
       "      <td>0.04304</td>\n",
       "      <td>0.1537</td>\n",
       "      <td>0.06171</td>\n",
       "      <td>...</td>\n",
       "      <td>15.300</td>\n",
       "      <td>33.17</td>\n",
       "      <td>100.20</td>\n",
       "      <td>706.7</td>\n",
       "      <td>0.12410</td>\n",
       "      <td>0.22640</td>\n",
       "      <td>0.1326</td>\n",
       "      <td>0.10480</td>\n",
       "      <td>0.2250</td>\n",
       "      <td>0.08321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>561</td>\n",
       "      <td>11.20</td>\n",
       "      <td>29.37</td>\n",
       "      <td>70.67</td>\n",
       "      <td>386.0</td>\n",
       "      <td>0.07449</td>\n",
       "      <td>0.03558</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.1060</td>\n",
       "      <td>0.05502</td>\n",
       "      <td>...</td>\n",
       "      <td>11.920</td>\n",
       "      <td>38.30</td>\n",
       "      <td>75.19</td>\n",
       "      <td>439.6</td>\n",
       "      <td>0.09267</td>\n",
       "      <td>0.05494</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.1566</td>\n",
       "      <td>0.05905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>568</td>\n",
       "      <td>7.76</td>\n",
       "      <td>24.54</td>\n",
       "      <td>47.92</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.05263</td>\n",
       "      <td>0.04362</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.1587</td>\n",
       "      <td>0.05884</td>\n",
       "      <td>...</td>\n",
       "      <td>9.456</td>\n",
       "      <td>30.37</td>\n",
       "      <td>59.16</td>\n",
       "      <td>268.6</td>\n",
       "      <td>0.08996</td>\n",
       "      <td>0.06444</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>0.07039</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>427 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        2      3       4       5        6        7        8        9       10  \\\n",
       "121  18.66  17.12  121.40  1077.0  0.10540  0.11000  0.14570  0.08665  0.1966   \n",
       "122  24.25  20.20  166.20  1761.0  0.14470  0.28670  0.42680  0.20120  0.2655   \n",
       "126  13.61  24.69   87.76   572.6  0.09258  0.07862  0.05285  0.03085  0.1761   \n",
       "127  19.00  18.91  123.40  1138.0  0.08217  0.08028  0.09271  0.05627  0.1946   \n",
       "129  19.79  25.12  130.40  1192.0  0.10150  0.15890  0.25450  0.11490  0.2202   \n",
       "..     ...    ...     ...     ...      ...      ...      ...      ...     ...   \n",
       "558  14.59  22.68   96.39   657.1  0.08473  0.13300  0.10290  0.03736  0.1454   \n",
       "559  11.51  23.93   74.52   403.5  0.09261  0.10210  0.11120  0.04105  0.1388   \n",
       "560  14.05  27.15   91.38   600.4  0.09929  0.11260  0.04462  0.04304  0.1537   \n",
       "561  11.20  29.37   70.67   386.0  0.07449  0.03558  0.00000  0.00000  0.1060   \n",
       "568   7.76  24.54   47.92   181.0  0.05263  0.04362  0.00000  0.00000  0.1587   \n",
       "\n",
       "          11  ...      22     23      24      25       26       27      28  \\\n",
       "121  0.06213  ...  22.250  24.90  145.40  1549.0  0.15030  0.22910  0.3272   \n",
       "122  0.06877  ...  26.020  23.99  180.90  2073.0  0.16960  0.42440  0.5803   \n",
       "126  0.06130  ...  16.890  35.64  113.20   848.7  0.14710  0.28840  0.3796   \n",
       "127  0.05044  ...  22.320  25.73  148.20  1538.0  0.10210  0.22640  0.3207   \n",
       "129  0.06113  ...  22.630  33.58  148.70  1589.0  0.12750  0.38610  0.5673   \n",
       "..       ...  ...     ...    ...     ...     ...      ...      ...     ...   \n",
       "558  0.06147  ...  15.480  27.27  105.90   733.5  0.10260  0.31710  0.3662   \n",
       "559  0.06570  ...  12.480  37.16   82.28   474.2  0.12980  0.25170  0.3630   \n",
       "560  0.06171  ...  15.300  33.17  100.20   706.7  0.12410  0.22640  0.1326   \n",
       "561  0.05502  ...  11.920  38.30   75.19   439.6  0.09267  0.05494  0.0000   \n",
       "568  0.05884  ...   9.456  30.37   59.16   268.6  0.08996  0.06444  0.0000   \n",
       "\n",
       "          29      30       31  \n",
       "121  0.16740  0.2894  0.08456  \n",
       "122  0.22480  0.3222  0.08009  \n",
       "126  0.13290  0.3470  0.07900  \n",
       "127  0.12180  0.2841  0.06541  \n",
       "129  0.17320  0.3305  0.08465  \n",
       "..       ...     ...      ...  \n",
       "558  0.11050  0.2258  0.08004  \n",
       "559  0.09653  0.2112  0.08732  \n",
       "560  0.10480  0.2250  0.08321  \n",
       "561  0.00000  0.1566  0.05905  \n",
       "568  0.00000  0.2871  0.07039  \n",
       "\n",
       "[427 rows x 30 columns]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xTest = positiveDataset.iloc[:71, 1:].append(negativeeDataset.iloc[:71, 1:])\n",
    "yTest = positiveDataset.iloc[:71, 0].append(negativeeDataset.iloc[:71, 0])\n",
    "\n",
    "xTrain = positiveDataset.iloc[71:, 1:].append(negativeeDataset.iloc[71:, 1:])\n",
    "yTrain = positiveDataset.iloc[71:, 0].append(negativeeDataset.iloc[71:, 0])\n",
    "xTrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-20T01:31:43.942703Z",
     "start_time": "2019-11-20T01:31:43.916568Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>...</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>17.990</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.30010</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>25.380</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.16220</td>\n",
       "      <td>0.66560</td>\n",
       "      <td>0.71190</td>\n",
       "      <td>0.26540</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>20.570</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.08690</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>24.990</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.12380</td>\n",
       "      <td>0.18660</td>\n",
       "      <td>0.24160</td>\n",
       "      <td>0.18600</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>19.690</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.19740</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>23.570</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.14440</td>\n",
       "      <td>0.42450</td>\n",
       "      <td>0.45040</td>\n",
       "      <td>0.24300</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>11.420</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.24140</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>14.910</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.20980</td>\n",
       "      <td>0.86630</td>\n",
       "      <td>0.68690</td>\n",
       "      <td>0.25750</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>20.290</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.19800</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>22.540</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.13740</td>\n",
       "      <td>0.20500</td>\n",
       "      <td>0.40000</td>\n",
       "      <td>0.16250</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>149</td>\n",
       "      <td>13.740</td>\n",
       "      <td>17.91</td>\n",
       "      <td>88.12</td>\n",
       "      <td>585.0</td>\n",
       "      <td>0.07944</td>\n",
       "      <td>0.06376</td>\n",
       "      <td>0.02881</td>\n",
       "      <td>0.01329</td>\n",
       "      <td>0.1473</td>\n",
       "      <td>0.05580</td>\n",
       "      <td>...</td>\n",
       "      <td>15.340</td>\n",
       "      <td>22.46</td>\n",
       "      <td>97.19</td>\n",
       "      <td>725.9</td>\n",
       "      <td>0.09711</td>\n",
       "      <td>0.18240</td>\n",
       "      <td>0.15640</td>\n",
       "      <td>0.06019</td>\n",
       "      <td>0.2350</td>\n",
       "      <td>0.07014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>13.000</td>\n",
       "      <td>20.78</td>\n",
       "      <td>83.51</td>\n",
       "      <td>519.4</td>\n",
       "      <td>0.11350</td>\n",
       "      <td>0.07589</td>\n",
       "      <td>0.03136</td>\n",
       "      <td>0.02645</td>\n",
       "      <td>0.2540</td>\n",
       "      <td>0.06087</td>\n",
       "      <td>...</td>\n",
       "      <td>14.160</td>\n",
       "      <td>24.11</td>\n",
       "      <td>90.82</td>\n",
       "      <td>616.7</td>\n",
       "      <td>0.12970</td>\n",
       "      <td>0.11050</td>\n",
       "      <td>0.08112</td>\n",
       "      <td>0.06296</td>\n",
       "      <td>0.3196</td>\n",
       "      <td>0.06435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>151</td>\n",
       "      <td>8.219</td>\n",
       "      <td>20.70</td>\n",
       "      <td>53.27</td>\n",
       "      <td>203.9</td>\n",
       "      <td>0.09405</td>\n",
       "      <td>0.13050</td>\n",
       "      <td>0.13210</td>\n",
       "      <td>0.02168</td>\n",
       "      <td>0.2222</td>\n",
       "      <td>0.08261</td>\n",
       "      <td>...</td>\n",
       "      <td>9.092</td>\n",
       "      <td>29.72</td>\n",
       "      <td>58.08</td>\n",
       "      <td>249.8</td>\n",
       "      <td>0.16300</td>\n",
       "      <td>0.43100</td>\n",
       "      <td>0.53810</td>\n",
       "      <td>0.07879</td>\n",
       "      <td>0.3322</td>\n",
       "      <td>0.14860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>152</td>\n",
       "      <td>9.731</td>\n",
       "      <td>15.34</td>\n",
       "      <td>63.78</td>\n",
       "      <td>300.2</td>\n",
       "      <td>0.10720</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.41080</td>\n",
       "      <td>0.07857</td>\n",
       "      <td>0.2548</td>\n",
       "      <td>0.09296</td>\n",
       "      <td>...</td>\n",
       "      <td>11.020</td>\n",
       "      <td>19.49</td>\n",
       "      <td>71.04</td>\n",
       "      <td>380.5</td>\n",
       "      <td>0.12920</td>\n",
       "      <td>0.27720</td>\n",
       "      <td>0.82160</td>\n",
       "      <td>0.15710</td>\n",
       "      <td>0.3108</td>\n",
       "      <td>0.12590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>153</td>\n",
       "      <td>11.150</td>\n",
       "      <td>13.08</td>\n",
       "      <td>70.87</td>\n",
       "      <td>381.9</td>\n",
       "      <td>0.09754</td>\n",
       "      <td>0.05113</td>\n",
       "      <td>0.01982</td>\n",
       "      <td>0.01786</td>\n",
       "      <td>0.1830</td>\n",
       "      <td>0.06105</td>\n",
       "      <td>...</td>\n",
       "      <td>11.990</td>\n",
       "      <td>16.30</td>\n",
       "      <td>76.25</td>\n",
       "      <td>440.8</td>\n",
       "      <td>0.13410</td>\n",
       "      <td>0.08971</td>\n",
       "      <td>0.07116</td>\n",
       "      <td>0.05506</td>\n",
       "      <td>0.2859</td>\n",
       "      <td>0.06772</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>142 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         2      3       4       5        6        7        8        9   \\\n",
       "0    17.990  10.38  122.80  1001.0  0.11840  0.27760  0.30010  0.14710   \n",
       "1    20.570  17.77  132.90  1326.0  0.08474  0.07864  0.08690  0.07017   \n",
       "2    19.690  21.25  130.00  1203.0  0.10960  0.15990  0.19740  0.12790   \n",
       "3    11.420  20.38   77.58   386.1  0.14250  0.28390  0.24140  0.10520   \n",
       "4    20.290  14.34  135.10  1297.0  0.10030  0.13280  0.19800  0.10430   \n",
       "..      ...    ...     ...     ...      ...      ...      ...      ...   \n",
       "149  13.740  17.91   88.12   585.0  0.07944  0.06376  0.02881  0.01329   \n",
       "150  13.000  20.78   83.51   519.4  0.11350  0.07589  0.03136  0.02645   \n",
       "151   8.219  20.70   53.27   203.9  0.09405  0.13050  0.13210  0.02168   \n",
       "152   9.731  15.34   63.78   300.2  0.10720  0.15990  0.41080  0.07857   \n",
       "153  11.150  13.08   70.87   381.9  0.09754  0.05113  0.01982  0.01786   \n",
       "\n",
       "         10       11  ...      22     23      24      25       26       27  \\\n",
       "0    0.2419  0.07871  ...  25.380  17.33  184.60  2019.0  0.16220  0.66560   \n",
       "1    0.1812  0.05667  ...  24.990  23.41  158.80  1956.0  0.12380  0.18660   \n",
       "2    0.2069  0.05999  ...  23.570  25.53  152.50  1709.0  0.14440  0.42450   \n",
       "3    0.2597  0.09744  ...  14.910  26.50   98.87   567.7  0.20980  0.86630   \n",
       "4    0.1809  0.05883  ...  22.540  16.67  152.20  1575.0  0.13740  0.20500   \n",
       "..      ...      ...  ...     ...    ...     ...     ...      ...      ...   \n",
       "149  0.1473  0.05580  ...  15.340  22.46   97.19   725.9  0.09711  0.18240   \n",
       "150  0.2540  0.06087  ...  14.160  24.11   90.82   616.7  0.12970  0.11050   \n",
       "151  0.2222  0.08261  ...   9.092  29.72   58.08   249.8  0.16300  0.43100   \n",
       "152  0.2548  0.09296  ...  11.020  19.49   71.04   380.5  0.12920  0.27720   \n",
       "153  0.1830  0.06105  ...  11.990  16.30   76.25   440.8  0.13410  0.08971   \n",
       "\n",
       "          28       29      30       31  \n",
       "0    0.71190  0.26540  0.4601  0.11890  \n",
       "1    0.24160  0.18600  0.2750  0.08902  \n",
       "2    0.45040  0.24300  0.3613  0.08758  \n",
       "3    0.68690  0.25750  0.6638  0.17300  \n",
       "4    0.40000  0.16250  0.2364  0.07678  \n",
       "..       ...      ...     ...      ...  \n",
       "149  0.15640  0.06019  0.2350  0.07014  \n",
       "150  0.08112  0.06296  0.3196  0.06435  \n",
       "151  0.53810  0.07879  0.3322  0.14860  \n",
       "152  0.82160  0.15710  0.3108  0.12590  \n",
       "153  0.07116  0.05506  0.2859  0.06772  \n",
       "\n",
       "[142 rows x 30 columns]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xTest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Monte-Carlo Simulation: Repeat the following procedures for supervised, unsupervised, and semi-supervised learning M = 30 times, and use randomly selected train and test data (make sure you use 20% of both the positve and negative classes as the test set). Then compare the average scores (accuracy, precision, recall, F-score, and AUC) that you obtain from each algorithm.\n",
    "#### i. Supervised Learning: Train an L1-penalized SVM to classify the data. Use 5 fold cross validation to choose the penalty parameter. Use normalized data. Report the average accuracy, precision, recall, F-score, and AUC, for both training and test sets over your M runs. Plot the ROC and report the confusion matrix for training and testing in one of the runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-20T01:31:43.949250Z",
     "start_time": "2019-11-20T01:31:43.945230Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-20T01:31:43.957372Z",
     "start_time": "2019-11-20T01:31:43.951225Z"
    }
   },
   "outputs": [],
   "source": [
    "def random_split(positiveDataset, negativeeDataset, testSize):\n",
    "    x_train1, x_test1, y_train1, y_test1 = train_test_split(positiveDataset.iloc[:, 1:], positiveDataset.iloc[:, 0], test_size=testSize, random_state=50)\n",
    "    x_train2, x_test2, y_train2, y_test2 = train_test_split(negativeeDataset.iloc[:, 1:], negativeeDataset.iloc[:, 0], test_size=testSize, random_state=50)\n",
    "    x_train = x_train1.append(x_train2)\n",
    "    y_train = y_train1.append(y_train2)\n",
    "    x_test = x_test1.append(x_test2)\n",
    "    y_test = y_test1.append(y_test2)\n",
    "    \n",
    "    x_train.reset_index(drop=True, inplace=True)\n",
    "    y_train.reset_index(drop=True, inplace=True)\n",
    "    x_test.reset_index(drop=True, inplace=True)\n",
    "    y_test.reset_index(drop=True, inplace=True)\n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-20T01:31:43.970015Z",
     "start_time": "2019-11-20T01:31:43.959143Z"
    }
   },
   "outputs": [],
   "source": [
    "def supervised_learning(x_train, y_train, x_test, y_test):\n",
    "    c_list = [10**c for c in np.arange(-3,7, dtype=float)]\n",
    "    parameters = {'C':c_list}\n",
    "    svc = LinearSVC(penalty='l1', dual=False)\n",
    "    clf = GridSearchCV(svc, parameters, cv=5)\n",
    "    clf.fit(x_train, y_train)\n",
    "    best_C = clf.best_params_['C']\n",
    "    # build L1 svm model\n",
    "    svc = LinearSVC(penalty='l1', dual=False, C=best_C)\n",
    "    svc.fit(x_train, y_train)\n",
    "    \n",
    "    # avg of accuracy, precision, recall, F-score, AUC\n",
    "    # accuracy\n",
    "    train_accuracy = svc.score(x_train, y_train)\n",
    "    test_accuracy = svc.score(x_test, y_test)\n",
    "    # confusion matrix\n",
    "    y_train_predict = svc.predict(x_train)\n",
    "    y_test_predict = svc.predict(x_test)\n",
    "    train_confusion_matrix = confusion_matrix(y_train, y_train_predict)\n",
    "    test_confusion_matrix = confusion_matrix(y_test, y_test_predict)\n",
    "    \n",
    "    # get parameter\n",
    "    train_tn, train_fp, train_fn, train_tp = train_confusion_matrix.ravel()\n",
    "    test_tn, test_fp, test_fn, test_tp = test_confusion_matrix.ravel()\n",
    "    # precision: p = tp/(tp+fp)\n",
    "    train_precision = train_tp/(train_tp + train_fp)    \n",
    "    test_precision = test_tp/(test_tp + test_fp)\n",
    "\n",
    "    # recall: tp/(tp+fn)\n",
    "    train_recall = train_tp/(train_tp + train_fn)\n",
    "    test_recall = test_tp/(test_tp + test_fn)\n",
    "    \n",
    "    # f1 = 2 * (P*R)/(P+R)\n",
    "    train_f1 = 2 * (train_precision * train_recall) / (train_precision + train_recall)\n",
    "    test_f1 = 2 * (test_precision * test_recall) / (test_precision + test_recall)\n",
    "    # AUC\n",
    "    train_predict_prob = svc.decision_function(x_train)\n",
    "    test_predict_prob = svc.decision_function(x_test)\n",
    "    train_auc = roc_auc_score(y_train, train_predict_prob)\n",
    "    test_auc = roc_auc_score(y_test, test_predict_prob)\n",
    "    \n",
    "#   ['Train Accuracy', 'Train Precision', 'Train Recall', 'Train F-Score', 'Train Auc', 'Test Accuracy',  'Test Precision', 'Test Recall', 'Test F-Score', 'Test Auc']\n",
    "    return [train_accuracy, train_precision, train_recall, train_f1, train_auc, test_accuracy, test_precision, test_recall, test_f1, test_auc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-20T01:32:17.800359Z",
     "start_time": "2019-11-20T01:31:43.973334Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVG Score:\n",
      " Train Accuracy     0.991630\n",
      "Train Precision    0.994378\n",
      "Train Recall       0.983037\n",
      "Train F-Score      0.988666\n",
      "Train Auc          0.999738\n",
      "Test Accuracy      0.953333\n",
      "Test Precision     0.964209\n",
      "Test Recall        0.909302\n",
      "Test F-Score       0.935680\n",
      "Test Auc           0.964966\n",
      "dtype: float64\n",
      "\n",
      "Train Confusion Matrix:\n",
      " [[285   0]\n",
      " [  1 168]]\n",
      "Test Confusion Matrix:\n",
      " [[71  1]\n",
      " [ 5 38]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3de3gV1dn38e8tB0FBfRVUBGkCRCUCIkYE5ZX2lSLgI1BFg4W2WpVKH6yHKtKWq7Xa+iK1Dx7AtlRAa0U8gVILRVRqrZRwRiEUoYgSoYp4RsIh3s8fM6SbsLOzEzI7ZM/vc1252DOz9sy9kpB7r7Vm1jJ3R0RE4uuwug5ARETqlhKBiEjMKRGIiMScEoGISMwpEYiIxFzDug6gulq0aOE5OTl1HYaISL2ybNmyD9y9ZbJj9S4R5OTksHTp0roOQ0SkXjGztys7pq4hEZGYUyIQEYk5JQIRkZhTIhARiTklAhGRmIssEZjZVDN738xWV3LczOx+M9tgZq+bWbeoYhERkcpF2SJ4GOiX4nh/IC/8GgH8JsJYRESkEpE9R+DufzOznBRFBgF/8GAe7EVmdoyZtXL3rVHFBDC96B2eW/lulJcQiZ0LvpjDeTsX1HUYWe+zYzrS4/u/r/Xz1uUDZa2BzQnbJeG+AxKBmY0gaDXQtm3bGl1sXwIoeutDAM7JPbZG5xGRA523cwE5ezayqVG7ug5FaqAuE4El2Zd0lRx3nwxMBigoKKjRSjrPrXyX4q2fck7usQzq2ppvnlOzhCIiSUw7GjiT06/6c11HIjVQl4mgBDg5YbsNsCXKC+a3OoonvtczykuIiNQ7dXn76Gzg2+HdQz2AT6IeHxARkQNF1iIws8eBrwItzKwE+BnQCMDdfwvMAQYAG4AvgKuiikWkSkunwRtP13UU9de/34ATO9d1FFJDUd41dEUVxx3476iuL1ItbzytP2YH48TO0HlIXUchNVTvpqEWicyJnUGDnRJDmmJCRCTm1CKQ9GVzP7q6hSTG1CKQ9O3rR89G6uOWGFOLQKpH/egiWUctAhGRmFMiEBGJOXUN1QeHyiCtBlRFspJaBPXBoTJIqwFVkaykFkF9oUFaEYmIWgQiIjGnRCAiEnPx7ho6VAZhq6JBWhGJULxbBIfKIGxVNEgrIhGKd4sANAgrIrEX7xaBiIgoEYiIxJ0SgYhIzCkRiIjEnBKBiEjMKRGIiMRcbG4fveCLOZy3cwFMO/o/O/WglohIfFoE5+1cQM6ejfvv1INaIiLxaREAbGrUjtP18JiIyH5i0yIQEZHklAhERGJOiUBEJOaUCEREYk6JQEQk5pQIRERiTolARCTmIk0EZtbPzNaZ2QYzG5PkeFszW2BmK8zsdTMbEGU8IiJyoMgSgZk1ACYB/YF84Aozy69QbCzwpLufCQwFHowqHhERSS7KFkF3YIO7b3T33cAMYFCFMg4cFb4+GtgSYTwiIpJElImgNbA5Ybsk3JfodmC4mZUAc4Drk53IzEaY2VIzW7pt27YoYhURia0oE4El2ecVtq8AHnb3NsAA4FEzOyAmd5/s7gXuXtCyZcsIQhURia8oE0EJcHLCdhsO7Pq5GngSwN3/ATQBWkQYk4iIVBBlIlgC5JlZrpk1JhgMnl2hzDvABQBm1pEgEajvR0QkgyJLBO6+FxgFzAPWEtwdtMbM7jCzgWGxHwLXmtkq4HHgSnev2H0kIiIRinQ9AnefQzAInLjvpwmvi4HzooxBRERS05PFIiIxp0QgIhJzSgQiIjGnRCAiEnNKBCIiMadEICISc0oEIiIxp0QgIhJzSgQiIjGXViIws8Zm1iHqYEREJPOqTARmdhHwBjA/3O5qZrOiDkxERDIjnRbBHcA5wMcA7r4SUOtARCRLpJMI9rj7xxX2aYZQEZEskc7so2vN7HLgMDPLBW4AFkUbloiIZEo6LYJRwFnAl8BMoJQgGYiISBZIp0VwobvfBty2b4eZXUKQFEREpJ5Lp0UwNsm+n9R2ICIiUjcqbRGY2YVAP6C1mf1PwqGjCLqJREQkC6TqGnofWE0wJrAmYf9nwJgogxIRkcypNBG4+wpghZk95u6lGYxJREQyKJ3B4tZm9ksgH2iyb6e7nxJZVCIikjHpDBY/DEwDDOgPPAnMiDAmERHJoHQSwRHuPg/A3f/l7mOBr0UbloiIZEo6XUO7zMyAf5nZdcC7wPHRhiUiIpmSTiK4CWgG/AD4JXA08N0ogxIRkcypMhG4e1H48jPgWwBm1ibKoEREJHNSjhGY2dlmNtjMWoTbp5vZH9CkcyIiWaPSRGBm/x94DBgG/MXMfgIsAFYBunVURCRLpOoaGgSc4e47zexYYEu4vS4zoYmISCak6hoqdfedAO7+IfBPJQERkeyTqkXQzsz2TTVtQE7CNu5+SVUnN7N+wH1AA+Ahdx+XpMzlwO0Eq56tcvdvph++iIgcrFSJ4NIK2xOrc2IzawBMAr4OlABLzGy2uxcnlMkDfgSc5+4fmZmeTxARybBUk869dJDn7g5scPeNAGY2g2DcoTihzLXAJHf/KLzm+wd5TRERqaZ0ppioqdbA5oTtknBfolOAU8zsNTNbFHYlHcDMRpjZUjNbum3btojCFRGJpygTgSXZ5xW2GwJ5wFeBK4CHzOyYA97kPtndC9y9oGXLlrUeqIhInKWdCMzs8GqeuwQ4OWG7DcEtqBXLPOfue9z9LWAdQWIQEZEMqTIRmFl3M3sDWB9un2FmD6Rx7iVAnpnlmlljYCgwu0KZZwlnMg2fXj4F2FiN+EVE5CCl0yK4H/gvYDuAu68ijWmo3X0vMAqYB6wFnnT3NWZ2h5kNDIvNA7abWTHBU8u3uvv26ldDRERqKp3ZRw9z97eDmajLlaVzcnefA8ypsO+nCa8duDn8EhGROpBOIthsZt0BD58NuB54M9qwREQkU9LpGhpJ8Im9LfAe0CPcJyIiWSCdFsFedx8aeSQiIlIn0mkRLDGzOWb2HTNrHnlEIiKSUVUmAndvD/wCOAt4w8yeNTO1EEREskRaD5S5+0J3/wHQDfiUYMEaERHJAuk8UNbMzIaZ2Z+AxcA24NzIIxMRkYxIZ7B4NfAnYLy7vxpxPCIikmHpJIJ27v5l5JGIiEidqDQRmNmv3f2HwDNmVnHW0LRWKBMRkUNfqhbBE+G/1VqZTERE6pdUK5QtDl92dPf9koGZjQIOdgUzERE5BKRz++h3k+y7urYDERGRupFqjKCQYA2BXDObmXCoOfBx1IGJiEhmpBojWEywBkEbYFLC/s+AFVEGJSIimZNqjOAt4C3gxcyFIyIimZaqa+gVd+9tZh+x/6LzRrCmzLGRRyciIpFL1TW0bznKFpkIRERE6kaldw0lPE18MtDA3cuAnsD3gCMzEJuIiGRAOrePPkuwTGV74A9AR2B6pFGJiEjGpJMIvnT3PcAlwL3ufj3QOtqwREQkU9JJBHvN7DLgW8Dz4b5G0YUkIiKZlO6TxV8jmIZ6o5nlAo9HG5aIiGRKldNQu/tqM/sB0MHMTgM2uPsvow9NREQyocpEYGb/F3gUeJfgGYITzexb7v5a1MGJiEj00lmYZgIwwN2LAcysI0FiKIgyMBERyYx0xgga70sCAO6+FmgcXUgiIpJJ6bQIlpvZ7whaAQDD0KRzIiJZI51EcB3wA2A0wRjB34AHogxKREQyJ2UiMLPOQHtglruPz0xIIiKSSZWOEZjZjwmmlxgGzDezZCuViYhIPZdqsHgY0MXdLwPOBkZW9+Rm1s/M1pnZBjMbk6LcEDNzM9OdSCIiGZYqEexy9x0A7r6tirIHMLMGBCub9QfygSvMLD9JueYEYxBF1Tm/iIjUjlRjBO0S1io2oH3i2sXufkkV5+5O8BTyRgAzmwEMAoorlLsTGA/cUp3ARUSkdqRKBJdW2J5YzXO3BjYnbJcA5yQWMLMzgZPd/XkzqzQRmNkIYARA27ZtqxmGiIikkmrN4pcO8tyW7LTlB80OI3hq+cqqTuTuk4HJAAUFBV5FcRERqYZq9ftXUwnB6mb7tAG2JGw3BzoBfzWzTUAPYLYGjEVEMivKRLAEyDOzXDNrDAwFZu876O6fuHsLd89x9xxgETDQ3ZdGGJOIiFSQdiIws8Orc2J33wuMAuYBa4En3X2Nmd1hZgOrF6aIiEQlnWmouwNTgKOBtmZ2BnBNuGRlSu4+B5hTYd9PKyn71XQCFhGR2pVOi+B+4L+A7QDuvopgxTIREckC6SSCw9z97Qr7yqIIRkREMi+d2Uc3h91DHj4tfD3wZrRhiYhIpqTTIhgJ3Ay0Bd4juM2z2vMOiYjIoSmdxevfJ7j1U0REslA6dw39noQngvdx9xGRRCQiIhmVzhjBiwmvmwDfYP85hEREpB5Lp2voicRtM3sUmB9ZRCIiklE1mWIiF/hKbQciIiJ1I50xgo/4zxjBYcCHQKWrjYmISP1S1eL1BpwBvBvu+tLdNQ20iEgWSdk1FP7Rn+XuZeGXkoCISJZJZ4xgsZl1izwSERGpE5V2DZlZw3Aq6V7AtWb2L2AHwcpj7u5KDiIiWSDVGMFioBswOEOxiIhIHUiVCAzA3f+VoVhERKQOpEoELc3s5soOuvv/RBCPiIhkWKpE0ABoRtgyEBGR7JQqEWx19zsyFomIiNSJVLePqiUgIhIDqRLBBRmLQkRE6kylicDdP8xkICIiUjdqMvuoiIhkESUCEZGYUyIQEYk5JQIRkZhTIhARiTklAhGRmFMiEBGJOSUCEZGYizQRmFk/M1tnZhvM7IAF783sZjMrNrPXzewlM/tKlPGIiMiBIksEZtYAmAT0B/KBK8wsv0KxFUCBu3cBngbGRxWPiIgkF2WLoDuwwd03uvtuYAYwKLGAuy9w9y/CzUVAmwjjERGRJKJMBK2BzQnbJeG+ylwNzE12wMxGmNlSM1u6bdu2WgxRRESiTATJprH2pAXNhgMFwK+SHXf3ye5e4O4FLVu2rMUQRUQk1cI0B6sEODlhuw2wpWIhM+sD/ATo7e67IoxHRESSiLJFsATIM7NcM2sMDAVmJxYwszOB3wED3f39CGMREZFKRJYI3H0vMAqYB6wFnnT3NWZ2h5kNDIv9imBd5KfMbKWZza7kdCIiEpEou4Zw9znAnAr7fprwuk+U1xcRkarpyWIRkZhTIhARiTklAhGRmFMiEBGJOSUCEZGYUyIQEYm5SG8fFZFD3549eygpKaG0tLSuQ5Fa0KRJE9q0aUOjRo3Sfo8SgUjMlZSU0Lx5c3JycjBLNkWY1Bfuzvbt2ykpKSE3Nzft96lrSCTmSktLOe6445QEsoCZcdxxx1W7dadEICJKAlmkJj9LJQIRkZhTIhCROrV9+3a6du1K165dOfHEE2ndunX59u7du9M6x1VXXcW6devSvuZDDz1Ey5Yt6dq1K6eddhr333//fsd/85vfcNppp3HaaadxzjnnsHDhwvJje/bsYfTo0XTo0IFOnTpxzjnnMG/evLSvfSjSYLGI1KnjjjuOlStXAnD77bfTrFkzbrnllv3KuDvuzmGHJf/sOm3atGpfd9iwYdx7771s27aNU089lcsuu4xWrVrx7LPPMm3aNBYuXMixxx7L0qVLueSSS1i2bBktW7bkRz/6ER9++CHFxcU0btyYrVu38tprr1W/4imUlZXRoEGDWj1nKkoEIlLu539aQ/GWT2v1nPknHcXPLj692u/bsGEDgwcPplevXhQVFfH888/z85//nOXLl7Nz504KCwv56U+DyYx79erFxIkT6dSpEy1atOC6665j7ty5HHHEETz33HMcf/zxlV6nZcuWtGvXjq1bt9KqVSvuvvtu7rnnHo499lgACgoKGDZsGA8++CA333wzDz/8MJs2baJx48YAtGrViiFDhhxw3qKiIm688Ua++OILmjRpwoIFC5g+fTqrV6/m3nvvBaBfv36MHTuWHj160KJFC0aNGsULL7zAxRdfzNq1a5k+fToAL774IpMmTWLWrFnMnTuXO+64g127dpGXl8fUqVM58sgjq/39TaSuIRE5ZBUXF3P11VezYsUKWrduzbhx41i6dCmrVq1i/vz5FBcXH/CeTz75hN69e7Nq1Sp69uzJ1KlTU15j06ZNlJWV0alTp/JrnnXWWfuVKSgoYM2aNaxfv57c3FyaNWuW8pylpaUMHTqUSZMmsWrVKl544QUOP/zwlO/55JNP6NatG4sXL+a2227j1VdfZefOnQA88cQTFBYW8v777zNu3Dheeuklli9fTpcuXbjvvvtSnjcdahGISLmafHKPUvv27Tn77LPLtx9//HGmTJnC3r172bJlC8XFxeTn5+/3nqZNm9K/f38AzjrrLF599dWk537ssceYP38+69atY9q0aeWf8JNx92rdjbN27Vratm1Lt27dADj66KOrfE/jxo35xje+Uf7661//On/+858ZNGgQ8+bN49577y1Pfueeey4Au3fvplevXmnHVRklAhE5ZCV2eaxfv5777ruPxYsXc8wxxzB8+PCk98sn/kFv0KABe/fuTXrufWMEf//73xk4cCAXXnghxx9/PB07dmTZsmWcf/755WWXL19Ofn4+eXl5vPXWW+zYsSNld0xliaNhw4Z8+eWX5duJ8Tdt2nS/9xQWFjJlyhSOOOIIevbsyZFHHom7069fPx599NFKr10T6hoSkXrh008/pXnz5hx11FFs3bq11u7U6dWrF1dccQUPPPAAAKNHj2b06NF89NFHQJAE/vjHPzJy5EiaN2/Ot7/9bW688Ub27NkDwJYtW3jsscf2O+fpp5/O22+/zfLly8tjLysrIycnhxUrVuDubNq0iWXLllUa1wUXXEBRURFTpkyhsLAQgHPPPZdXXnmFjRs3ArBjxw7Wr19/0N8DtQhEpF7o1q0b+fn5dOrUiXbt2nHeeefV2rnHjBlD9+7dGTNmDJdccglbt26lR48emBlHHXUU06dPLx9wHjduHD/+8Y/p2LEjTZs25cgjj+TOO+/c73yHH344jz/+OCNHjqS0tJSmTZvy8ssv07t3b1q3bk3nzp3p1KkTXbt2rTSmhg0b0r9/f6ZPn16eaE444YTyxLDv1tq77rqLvLy8g6q/uftBnSDTCgoKfOnSpdV+35q7gn6003/899oOSaReW7t2LR07dqzrMKQWJfuZmtkydy9IVl5dQyIiMadEICISc0oEIiIxp0QgIhJzSgQiIjGnRCAiEnNKBCJSp2pjGmqAqVOn8u9//zvpseHDh5Obm0vXrl0544wzWLBgQfmxXbt2cf3119O+fXvy8vIYPHgwW7ZsKT++ZcsWLr/8cjp06EB+fj4XXXQRGzZsqHmFD0FKBCJSp/ZNQ71y5Uquu+46brrppvLtVPP/VJQqEQBMmDCBlStXcs899/D973+/fP9tt93Grl27ePPNN1m/fj0XXXQRl156KRBMFTF48GD69u3Lhg0bKC4u5s477+S9996reYUrqGwKjEzSk8Ui8h9zx8C/36jdc57YGfqPq9FbH3nkESZNmsTu3bs599xzmThxIl9++SVXXXUVK1euxN0ZMWIEJ5xwAitXrqSwsJCmTZuyePHiSpNIz549effddwH47LPP+OMf/8imTZvK5/+/9tprmTp1Kq+88gq7du2iWbNmXHPNNeXv3zeRXEXTpk1jwoQJmBndunVj2rRpDB8+nCFDhjB48GAAmjVrxueff86LL77IuHHjaNGiBWvWrKFv376ceuqpjBgxAoCxY8fSsmVLbrjhBsaNG8fMmTMpLS1lyJAh5VNv1yYlAhE5JK1evZpZs2axcOFCGjZsyIgRI5gxYwbt27fngw8+4I03goT18ccfc8wxx/DAAw8wceLElNM2APzlL38p/8Nc2bTS+6adLi0tPWBK6mRWrVrF3XffXb6YzYcffljlexYtWkRxcTFt27ZlyZIljBkzpjwRPPXUUyxYsIA5c+bwzjvvUFRUhLszYMAAFi5cWD77aG1RIhCR/6jhJ/covPjiiyxZsoSCgmBWhJ07d3LyySdz4YUXsm7dOm644QYGDBhA37590zrfTTfdxE033cQHH3zA4sWLgcpnCa3utNMvv/wyhYWF5YvZ7Ps3lZ49e9K2bVsAzj77bDZv3sx7771HSUkJJ554IieddBLjx49n7ty5nHnmmQB8/vnnvPnmm/UrEZhZP+A+oAHwkLuPq3D8cOAPwFnAdqDQ3TdFGZOI1A/uzne/+90DJnQDeP3115k7dy73338/zzzzDJMnT67yfBMmTODiiy9mwoQJXHnllRQVFXHKKaewceNGPv/88/1aBcuXL+eyyy6jtLSU559/Pq1Yq5p2uqysbL/xgIrTWF966aU888wzbNq0iaFDh5afd+zYsVx99dVVxnAwIhssNrMGwCSgP5APXGFm+RWKXQ185O4dgAnA3VHFIyL1S58+fXjyySf54IMPgODuonfeeYdt27bh7lx22WXlS1cCNG/enM8++yzlORs0aMAPf/hDvvjiC1566SWaN2/ON7/5TW699dbyP9hTp06lrKyM3r1707dvXz799NP9VjkrKio6YLGbPn36MGPGjPIuoX3/5uTklE81PWvWLMrKyiqNbejQocyYMYOZM2eWD1ZfeOGFTJkyhR07dgBQUlJS/v2oTVHeNdQd2ODuG919NzADGFShzCDgkfD108AFVp32mIhkrc6dO/Ozn/2MPn360KVLF/r27ct7773H5s2bOf/88+natSvXXnstd911FwBXXXUV11xzTZW3nZoZY8eOZfz48QCMHz+eww47jLy8PDp06MCzzz7LzJkzy8s+99xzzJkzh/bt29OpUyd+8YtfcNJJJ+13zi5dujB69OjyuG699VYAvve97zF//ny6d+/OypUrUy5XecYZZ7Bt2zZyc3PLp7weMGAAQ4YMoUePHnTu3JnLL7+czz//vObf1Mq+J1FNQ21mQ4B+7n5NuP0t4Bx3H5VQZnVYpiTc/ldY5oMK5xoBjABo27btWW+//Xa141n04LUA9Pj+72tUH5FspWmos091p6GOcowg2Sf7ilknnTK4+2RgMgTrEdQkGCUAEZHkouwaKgFOTthuA2yprIyZNQSOBqq+70pERGpNlIlgCZBnZrlm1hgYCsyuUGY28J3w9RDgZa9vS6aJZAH9t8seNflZRpYI3H0vMAqYB6wFnnT3NWZ2h5kNDItNAY4zsw3AzcCYqOIRkeSaNGnC9u3blQyygLuzfft2mjRpUq33xWbNYhFJbs+ePZSUlFBaWlrXoUgtaNKkCW3atKFRo0b77a+rwWIRqQcaNWpEbm5uXYchdUizj4qIxJwSgYhIzCkRiIjEXL0bLDazbUD1Hy0OtABqf6KOQ5vqHA+qczwcTJ2/4u4tkx2od4ngYJjZ0spGzbOV6hwPqnM8RFVndQ2JiMScEoGISMzFLRFUvXpF9lGd40F1jodI6hyrMQIRETlQ3FoEIiJSgRKBiEjMZWUiMLN+ZrbOzDaY2QEzmprZ4Wb2RHi8yMxyMh9l7UqjzjebWbGZvW5mL5nZV+oiztpUVZ0Tyg0xMzezen+rYTp1NrPLw5/1GjObnukYa1sav9ttzWyBma0If78H1EWctcXMpprZ++EKjsmOm5ndH34/Xjezbgd9UXfPqi+gAfAvoB3QGFgF5Fco833gt+HrocATdR13Bur8NeCI8PXIONQ5LNcc+BuwCCio67gz8HPOA1YA/yfcPr6u485AnScDI8PX+cCmuo77IOt8PtANWF3J8QHAXIIVHnsARQd7zWxsEXQHNrj7RnffDcwABlUoMwh4JHz9NHCBmSVbNrO+qLLO7r7A3b8INxcRrBhXn6Xzcwa4ExgPZMMcy+nU+Vpgkrt/BODu72c4xtqWTp0dOCp8fTQHroRYr7j730i9UuMg4A8eWAQcY2atDuaa2ZgIWgObE7ZLwn1Jy3iwgM4nwHEZiS4a6dQ50dUEnyjqsyrrbGZnAie7+/OZDCxC6fycTwFOMbPXzGyRmfXLWHTRSKfOtwPDzawEmANcn5nQ6kx1/79XKRvXI0j2yb7iPbLplKlP0q6PmQ0HCoDekUYUvZR1NrPDgAnAlZkKKAPS+Tk3JOge+ipBq+9VM+vk7h9HHFtU0qnzFcDD7v5rM+sJPBrW+cvow6sTtf73KxtbBCXAyQnbbTiwqVhexswaEjQnUzXFDnXp1Bkz6wP8BBjo7rsyFFtUqqpzc6AT8Fcz20TQlzq7ng8Yp/u7/Zy773H3t4B1BImhvkqnzlcDTwK4+z+AJgSTs2WrtP6/V0c2JoIlQJ6Z5ZpZY4LB4NkVyswGvhO+HgK87OEoTD1VZZ3DbpLfESSB+t5vDFXU2d0/cfcW7p7j7jkE4yID3b0+r3Oazu/2swQ3BmBmLQi6ijZmNMralU6d3wEuADCzjgSJYFtGo8ys2cC3w7uHegCfuPvWgzlh1nUNufteMxsFzCO442Cqu68xszuApe4+G5hC0HzcQNASGFp3ER+8NOv8K6AZ8FQ4Lv6Ouw+ss6APUpp1zipp1nke0NfMioEy4FZ33153UR+cNOv8Q+D3ZnYTQRfJlfX5g52ZPU7QtdciHPf4GdAIwN1/SzAOMgDYAHwBXHXQ16zH3y8REakF2dg1JCIi1aBEICISc0oEIiIxp0QgIhJzSgQiIjGnRCCHHDMrM7OVCV85KcrmVDZLYzWv+ddwhstV4fQMp9bgHNeZ2bfD11ea2UkJxx4ys/xajnOJmXVN4z03mtkRB3ttyV5KBHIo2unuXRO+NmXousPc/QyCCQl/Vd03u/tv3f0P4eaVwEkJx65x9+JaifI/cT5IenHeCCgRSKWUCKReCD/5v2pmy8Ovc5OUOd3MFoetiNfNLC/cPzxh/+/MrEEVl/sb0CF87wXhPPdvhPPEHx7uH2f/Wd/hnnDf7WZ2i5kNIZjP6bHwmk3DT/IFZjbSzMYnxHylmT1Qwzj/QcJkY2b2GzNbasE6BD8P9/2AICEtMLMF4b6+ZvaP8Pv4lJk1q+I6kuWUCORQ1DShW2hWuO994Ovu3g0oBO5P8r7rgPvcvSvBH+KScMqBQuC8cH8ZMKyK618MvGFmTYCHgUJ370zwJP5IMzsW+AZwurt3AX6R+GZ3fxpYSnq2A6AAAAJfSURBVPDJvau770w4/DRwScJ2IfBEDePsRzClxD4/cfcCoAvQ28y6uPv9BPPQfM3dvxZOOzEW6BN+L5cCN1dxHclyWTfFhGSFneEfw0SNgIlhn3gZwRw6Ff0D+ImZtQFmuvt6M7sAOAtYEk6t0ZQgqSTzmJntBDYRTGV8KvCWu78ZHn8E+G9gIsH6Bg+Z2Z+BtKe5dvdtZrYxnCNmfXiN18LzVifOIwmmXEhcnepyMxtB8P+6FcEiLa9XeG+PcP9r4XUaE3zfJMaUCKS+uAl4DziDoCV7wEIz7j7dzIqAi4B5ZnYNwZS9j7j7j9K4xrDESenMLOkaFeH8N90JJjobCowC/l816vIEcDnwT2CWu7sFf5XTjpNgpa5xwCTgEjPLBW4Bznb3j8zsYYLJ1yoyYL67X1GNeCXLqWtI6oujga3hHPPfIvg0vB8zawdsDLtDZhN0kbwEDDGz48Myx1r66zX/E8gxsw7h9reAV8I+9aPdfQ7BQGyyO3c+I5gKO5mZwGCCefSfCPdVK05330PQxdMj7FY6CtgBfGJmJwD9K4llEXDevjqZ2RFmlqx1JTGiRCD1xYPAd8xsEUG30I4kZQqB1Wa2EjiNYDm/YoI/mC+Y2evAfIJukyq5eynBzI5PmdkbwJfAbwn+qD4fnu8VgtZKRQ8Dv903WFzhvB8BxcBX3H1xuK/acYZjD78GbnH3VQRrFa8BphJ0N+0zGZhrZgvcfRvBHU2Ph9dZRPC9khjT7KMiIjGnFoGISMwpEYiIxJwSgYhIzCkRiIjEnBKBiEjMKRGIiMScEoGISMz9L04998flaeWLAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_train, y_train, x_test, y_test = random_split(positiveDataset, negativeeDataset, 0.2)\n",
    "normalized_x_train = pd.DataFrame(preprocessing.normalize(x_train))\n",
    "normalized_x_test = pd.DataFrame(preprocessing.normalize(x_test))\n",
    "cols = ['Train Accuracy', 'Train Precision', 'Train Recall', 'Train F-Score', 'Train Auc', 'Test Accuracy',  'Test Precision', 'Test Recall', 'Test F-Score', 'Test Auc']\n",
    "supervised_stat = pd.DataFrame(columns=cols, index=range(30))\n",
    "for i in range(30):\n",
    "    supervised_stat.loc[i] = supervised_learning(normalized_x_train, y_train, normalized_x_test, y_test)\n",
    "\n",
    "print('Spuervised AVG Score:\\n', supervised_stat.mean())\n",
    "\n",
    "# choose one to print and plot\n",
    "# get confusion matrix and plot roc\n",
    "c_list = [10**c for c in np.arange(-3,7, dtype=float)]\n",
    "parameters = {'C':c_list}\n",
    "svc = LinearSVC(penalty='l1', dual=False)\n",
    "clf = GridSearchCV(svc, parameters, cv=5)\n",
    "clf.fit(normalized_x_train, y_train)\n",
    "best_C = clf.best_params_['C']\n",
    "# build L1 svm model\n",
    "svc = LinearSVC(penalty='l1', dual=False, C=best_C)\n",
    "svc.fit(normalized_x_train, y_train)\n",
    "# confusion matrix\n",
    "y_train_predict = svc.predict(normalized_x_train)\n",
    "y_test_predict = svc.predict(normalized_x_test)\n",
    "train_confusion_matrix = confusion_matrix(y_train, y_train_predict)\n",
    "test_confusion_matrix = confusion_matrix(y_test, y_test_predict)\n",
    "print('\\nTrain Confusion Matrix:\\n', train_confusion_matrix)\n",
    "print('Test Confusion Matrix:\\n', test_confusion_matrix)\n",
    "# ROC\n",
    "train_predict_prob = svc.decision_function(normalized_x_train)\n",
    "test_predict_prob = svc.decision_function(normalized_x_test)\n",
    "train_fpr, train_tpr, train_thresholds = roc_curve(y_train, train_predict_prob)\n",
    "test_fpr, test_tpr, test_thresholds = roc_curve(y_test, test_predict_prob)\n",
    "# AUC\n",
    "train_predict_prob = svc.decision_function(normalized_x_train)\n",
    "test_predict_prob = svc.decision_function(normalized_x_test)\n",
    "train_auc = roc_auc_score(y_train, train_predict_prob)\n",
    "test_auc = roc_auc_score(y_test, test_predict_prob)\n",
    "\n",
    "plt.plot(train_fpr, train_tpr, label='Train ROC curve')\n",
    "plt.plot(test_fpr, test_tpr, label='Test ROC curve')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ii. Semi-Supervised Learning/ Self-training: select 50% of the positive class along with 50% of the negative class in the training set as labeled data and the rest as unlabelled data. You can select them randomly.\n",
    "#### A. Train an L1-penalized SVM to classify the labeled data Use normalized data. Choose the penalty parameter using 5 fold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-20T01:32:18.577915Z",
     "start_time": "2019-11-20T01:32:17.804219Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "          multi_class='ovr', penalty='l1', random_state=None, tol=0.0001,\n",
       "          verbose=0)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.concat([y_train, x_train], axis=1)\n",
    "label_x_train, label_y_train, unlabel_x_train, unlabel_y_train = random_split(train[train[1]==1], train[train[1]==0], 0.5)\n",
    "\n",
    "# choose the parameter\n",
    "c_list = [10**c for c in np.arange(-3,7, dtype=float)]\n",
    "parameters = {'C':c_list}\n",
    "svc = LinearSVC(penalty='l1', dual=False)\n",
    "clf = GridSearchCV(svc, parameters, cv=5)\n",
    "clf.fit(label_x_train, label_y_train)\n",
    "best_C = clf.best_params_['C']\n",
    "# build L1 svm model using best C\n",
    "svc = LinearSVC(penalty='l1', dual=False, C=best_C)\n",
    "svc.fit(label_x_train, label_y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B. Find the unlabeled data point that is the farthest to the decision boundary of the SVM. Let the SVM label it (ignore its true label), and add it to the labeled data, and retrain the SVM. Continue this process until all unlabeled data are used. Test the final SVM on the test data and the average accuracy, precision, recall, F-score, and AUC, for both training and test sets over your M runs. Plot the ROC and report the confusion matrix for training and testing in one of the runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-20T01:32:18.593869Z",
     "start_time": "2019-11-20T01:32:18.580870Z"
    }
   },
   "outputs": [],
   "source": [
    "def semi_supervised_learning(x_train, y_train, unlabel_x_train, x_test, y_test):\n",
    "    # use labeled data to train the model\n",
    "    c_list = [10**c for c in np.arange(-3,7, dtype=float)]\n",
    "    parameters = {'C':c_list}\n",
    "    svc = LinearSVC(penalty='l1', dual=False)\n",
    "    clf = GridSearchCV(svc, parameters, cv=5)\n",
    "    clf.fit(x_train, y_train)\n",
    "    best_C = clf.best_params_['C']\n",
    "    # build L1 svm model\n",
    "    svc = LinearSVC(penalty='l1', dual=False, C=best_C)\n",
    "    svc.fit(x_train, y_train)\n",
    "    \n",
    "    # unlabel_x_train as the unlabeled data\n",
    "    # find the farthest unlabeled data\n",
    "    unlabel_x = unlabel_x_train\n",
    "    while len(unlabel_x) > 0:\n",
    "        # calculate the distance of each data\n",
    "        distance_decision_boundary = svc.decision_function(unlabel_x)\n",
    "        abs_distance = np.abs(distance_decision_boundary)\n",
    "        # get the index of max distance\n",
    "        indx = abs_distance.argmax()\n",
    "        # get the label for farthest point\n",
    "        label_predict = 0 if distance_decision_boundary[indx] < 0 else 1\n",
    "        # append new label into train set\n",
    "        new_x_train = np.append(x_train, [unlabel_x.iloc[indx]], axis = 0)\n",
    "        new_y_train = y_train.append(pd.Series(label_predict))\n",
    "        # delete label from test set\n",
    "        unlabel_x = unlabel_x.drop(unlabel_x.index[indx])\n",
    "        # refit the model\n",
    "        svc.fit(new_x_train, new_y_train)\n",
    "    \n",
    "    # avg of accuracy, precision, recall, F-score, AUC\n",
    "    # accuracy\n",
    "    train_accuracy = svc.score(new_x_train, new_y_train)\n",
    "    test_accuracy = svc.score(x_test, y_test)\n",
    "    # confusion matrix\n",
    "    y_train_predict = svc.predict(new_x_train)\n",
    "    y_test_predict = svc.predict(x_test)\n",
    "    train_confusion_matrix = confusion_matrix(new_y_train, y_train_predict)\n",
    "    test_confusion_matrix = confusion_matrix(y_test, y_test_predict)\n",
    "    \n",
    "    # get parameter\n",
    "    train_tn, train_fp, train_fn, train_tp = train_confusion_matrix.ravel()\n",
    "    test_tn, test_fp, test_fn, test_tp = test_confusion_matrix.ravel()\n",
    "    # precision: p = tp/(tp+fp)\n",
    "    train_precision = train_tp/(train_tp + train_fp)    \n",
    "    test_precision = test_tp/(test_tp + test_fp)\n",
    "\n",
    "    # recall: tp/(tp+fn)\n",
    "    train_recall = train_tp/(train_tp + train_fn)\n",
    "    test_recall = test_tp/(test_tp + test_fn)\n",
    "    \n",
    "    # f1 = 2 * (P*R)/(P+R)\n",
    "    train_f1 = 2 * (train_precision * train_recall) / (train_precision + train_recall)\n",
    "    test_f1 = 2 * (test_precision * test_recall) / (test_precision + test_recall)\n",
    "    # AUC\n",
    "    train_predict_prob = svc.decision_function(new_x_train)\n",
    "    test_predict_prob = svc.decision_function(x_test)\n",
    "    train_auc = roc_auc_score(new_y_train, train_predict_prob)\n",
    "    test_auc = roc_auc_score(y_test, test_predict_prob)\n",
    "    \n",
    "    stat = [train_accuracy, train_precision, train_recall, train_f1, train_auc, test_accuracy, test_precision, test_recall, test_f1, test_auc]\n",
    "#   ['Train Accuracy', 'Train Precision', 'Train Recall', 'Train F-Score', 'Train Auc', 'Test Accuracy',  'Test Precision', 'Test Recall', 'Test F-Score', 'Test Auc']\n",
    "    return stat, svc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-20T01:32:18.598608Z",
     "start_time": "2019-11-20T01:32:18.596154Z"
    }
   },
   "outputs": [],
   "source": [
    "# stat, semi_supervised_model = semi_supervised_learning(label_x_train, label_y_train, unlabel_x_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-20T01:37:43.765440Z",
     "start_time": "2019-11-20T01:35:12.493463Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semi Supervised AVG Score:\n",
      " Train Accuracy     0.991189\n",
      "Train Precision    0.998039\n",
      "Train Recall       0.978198\n",
      "Train F-Score      0.987999\n",
      "Train Auc          0.999834\n",
      "Test Accuracy      0.947826\n",
      "Test Precision     0.974359\n",
      "Test Recall        0.883721\n",
      "Test F-Score       0.926829\n",
      "Test Auc           0.956632\n",
      "dtype: float64\n",
      "\n",
      "Train Confusion Matrix:\n",
      " [[285   0]\n",
      " [169   0]]\n",
      "Test Confusion Matrix:\n",
      " [[72  0]\n",
      " [43  0]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cols = ['Train Accuracy', 'Train Precision', 'Train Recall', 'Train F-Score', 'Train Auc', 'Test Accuracy',  'Test Precision', 'Test Recall', 'Test F-Score', 'Test Auc']\n",
    "semi_stat = pd.DataFrame(columns=cols, index=range(30))\n",
    "semi_supervised_model = LinearSVC(penalty='l1', dual=False)\n",
    "for i in range(30):\n",
    "    stat, semi_supervised_model = semi_supervised_learning(label_x_train, label_y_train, unlabel_x_train, x_test, y_test)\n",
    "    semi_stat.loc[i] = stat\n",
    "\n",
    "print('Semi Supervised AVG Score:\\n', semi_stat.mean())\n",
    "\n",
    "# choose one to print and plot\n",
    "# confusion matrix\n",
    "y_train_predict = semi_supervised_model.predict(normalized_x_train)\n",
    "y_test_predict = semi_supervised_model.predict(normalized_x_test)\n",
    "train_confusion_matrix = confusion_matrix(y_train, y_train_predict)\n",
    "test_confusion_matrix = confusion_matrix(y_test, y_test_predict)\n",
    "print('\\nTrain Confusion Matrix:\\n', train_confusion_matrix)\n",
    "print('Test Confusion Matrix:\\n', test_confusion_matrix)\n",
    "# ROC\n",
    "train_predict_prob = semi_supervised_model.decision_function(normalized_x_train)\n",
    "test_predict_prob = semi_supervised_model.decision_function(normalized_x_test)\n",
    "train_fpr, train_tpr, train_thresholds = roc_curve(y_train, train_predict_prob)\n",
    "test_fpr, test_tpr, test_thresholds = roc_curve(y_test, test_predict_prob)\n",
    "# AUC\n",
    "train_predict_prob = semi_supervised_model.decision_function(normalized_x_train)\n",
    "test_predict_prob = semi_supervised_model.decision_function(normalized_x_test)\n",
    "train_auc = roc_auc_score(y_train, train_predict_prob)\n",
    "test_auc = roc_auc_score(y_test, test_predict_prob)\n",
    "\n",
    "plt.plot(train_fpr, train_tpr, label='Train ROC curve')\n",
    "plt.plot(test_fpr, test_tpr, label='Test ROC curve')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### iii. Unsupervised Learning: Run k-means algorithm on the whole training set. Ignore the labels of the data, and assume k = 2.\n",
    "#### A. Run the k-means algorithm multiple times. Make sure that you initialize the algoritm randomly. How do you make sure that the algorithm was not trapped in a local minimum?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-20T01:34:44.938477Z",
     "start_time": "2019-11-20T01:34:44.935990Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B. Compute the centers of the two clusters and find the closest 30 data points to each center. Read the true labels of those 30 data points and take a majority poll within them. The majority poll becomes the label predicted by k-means for the members of each cluster. Then compare the labels provided by k-means with the true labels of the training data and report the average accuracy, precision, recall, F-score, and AUC over M runs, and ROC and the confusion matrix for one of the runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-20T01:34:44.945635Z",
     "start_time": "2019-11-20T01:34:44.940612Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_label(x_cluster, y_cluster, clusterTag, center):\n",
    "    x_cluster = np.array(x_cluster)\n",
    "    y_cluster = np.array(y_cluster)\n",
    "    # get different cluster\n",
    "    cluster = x_cluster[clusterTag]\n",
    "    true_label = y_cluster[clusterTag]\n",
    "    disatance_array = np.empty(0)\n",
    "    \n",
    "    for point in cluster:\n",
    "        distance = np.linalg.norm(point - center)\n",
    "        disatance_array = np.append(disatance_array, distance)\n",
    "    # sort the distances\n",
    "    sorted_distances = disatance_array.argsort() \n",
    "    true_label = true_label[sorted_distances[:30]]\n",
    "    count = np.bincount(true_label)\n",
    "    classify_cluster = np.argmax(count)\n",
    "    return classify_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-20T01:34:44.959838Z",
     "start_time": "2019-11-20T01:34:44.948362Z"
    }
   },
   "outputs": [],
   "source": [
    "def k_means(x_train, y_train, x_test, y_test):\n",
    "    kmeans = KMeans(n_clusters=2, random_state=233).fit(x_train)\n",
    "    train_labels = kmeans.labels_\n",
    "    train_cluster_centers = kmeans.cluster_centers_\n",
    "    # get two clusters center\n",
    "    train_center_0 = train_cluster_centers[0]\n",
    "    train_center_1 = train_cluster_centers[1]\n",
    "    train_cluster_0 = np.where(train_labels==0)\n",
    "    train_cluster_1 = np.where(train_labels==1)\n",
    "\n",
    "    cluster_0 = get_label(x_train, y_train, train_cluster_0, train_center_0)\n",
    "    cluster_1 = get_label(x_train, y_train, train_cluster_1, train_center_1)\n",
    "\n",
    "    y_train_predict_labels = train_labels\n",
    "    y_train_predict_labels[cluster_0] = cluster_0\n",
    "    y_train_predict_labels[cluster_1] = cluster_1\n",
    "\n",
    "    train_accuracy = accuracy_score(y_train, y_train_predict_labels)\n",
    "\n",
    "    # get confusion matrix\n",
    "    train_confusion_matrix = confusion_matrix(y_train, y_train_predict_labels)\n",
    "\n",
    "    # test data\n",
    "    test_labels = kmeans.predict(x_test)\n",
    "    test_cluster_0 = np.where(test_labels==0)\n",
    "    test_cluster_1 = np.where(test_labels==1)\n",
    "    \n",
    "    test_predict_labels = test_labels\n",
    "    test_predict_labels[test_cluster_0] = cluster_0\n",
    "    test_predict_labels[test_cluster_1] = cluster_1\n",
    "    \n",
    "    test_accuracy = accuracy_score(y_test, test_predict_labels)\n",
    "    test_confusion_matrix = confusion_matrix(y_test, test_predict_labels)\n",
    "    # get parameter\n",
    "    train_tn, train_fp, train_fn, train_tp = train_confusion_matrix.ravel()\n",
    "    test_tn, test_fp, test_fn, test_tp = test_confusion_matrix.ravel()\n",
    "    # precision: p = tp/(tp+fp)\n",
    "    train_precision = train_tp/(train_tp + train_fp)    \n",
    "    test_precision = test_tp/(test_tp + test_fp)\n",
    "\n",
    "    # recall: tp/(tp+fn)\n",
    "    train_recall = train_tp/(train_tp + train_fn)\n",
    "    test_recall = test_tp/(test_tp + test_fn)\n",
    "    \n",
    "    # f1 = 2 * (P*R)/(P+R)\n",
    "    train_f1 = 2 * (train_precision * train_recall) / (train_precision + train_recall)\n",
    "    test_f1 = 2 * (test_precision * test_recall) / (test_precision + test_recall)\n",
    "    # AUC\n",
    "    train_auc = roc_auc_score(y_train, pd.DataFrame(kmeans.transform(x_train)).iloc[:,0])\n",
    "    test_auc = roc_auc_score(y_test, pd.DataFrame(kmeans.transform(x_test)).iloc[:,0])\n",
    "    \n",
    "    return [train_accuracy, train_precision, train_recall, train_f1, train_auc, test_accuracy, test_precision, test_recall, test_f1, test_auc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-20T01:34:46.541880Z",
     "start_time": "2019-11-20T01:34:44.961792Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K Means AVG Score:\n",
      " Train Accuracy     0.852423\n",
      "Train Precision    0.990385\n",
      "Train Recall       0.609467\n",
      "Train F-Score      0.754579\n",
      "Train Auc          0.913028\n",
      "Test Accuracy      0.826087\n",
      "Test Precision     1.000000\n",
      "Test Recall        0.534884\n",
      "Test F-Score       0.696970\n",
      "Test Auc           0.906977\n",
      "dtype: float64\n",
      "\n",
      "Train Confusion Matrix\n",
      " [[284   1]\n",
      " [ 66 103]]\n",
      "Test Confusion Matrix\n",
      " [[72  0]\n",
      " [20 23]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3deZhU5bXv8e+SQVBavQgqgtjNoIKAiC2DemNyNYh4FFQUFJJIVKK5OMaBJDxqMMeLxAQnkkgENCqCE8oxEETlGCPSTIJKcxCCCC0EGRQBaSbX/WPvboumurp62FVdXb/P8/RD7aH2XruBWvW+797rNXdHRESy1yHpDkBERNJLiUBEJMspEYiIZDklAhGRLKdEICKS5eqnO4DKatasmefm5qY7DBGRjLJo0aLN7t483raMSwS5ubksXLgw3WGIiGQUM/usvG3qGhIRyXJKBCIiWU6JQEQkyykRiIhkOSUCEZEsF1kiMLOJZvaFmX1cznYzs0fNbJWZfWhm3aKKRUREyhdli+ApoE+C7RcC7cOfYcCfIoxFRETKEdlzBO7+DzPLTbBLP+CvHtTBnmdmR5lZC3ffEFVMIlJDFk6Cj15KdxRpt3F7MZt37E7Z+bYf1YGeP/9LjR83nQ+UtQTWxSwXhesOSgRmNoyg1UDr1q1TEpyIJPDRS/Dvj+C4zumOBEj9B3KJ7cX7AMhplHHP5h4gndFbnHVxZ8lx9/HAeID8/HzNpCNSGxzXGYb+Ld1RAHDzE+9TuPVrOrY4IuXn7te1JVf3yOwvqOlMBEXACTHLrYD1aYpFRKppcsFaXlvyeVrOXbghSAJTf9YrLefPdOlMBNOB4WY2BegBbNP4gEjNifKD+Z4t2wAY9cT7pesKPt0KQI+8ppGcM5GOLY6gX9eWKT9vXRFZIjCz54HvA83MrAi4F2gA4O5/BmYAfYFVwDfA0KhiEckmJQkg1R/MPfKa1olukmwU5V1DV1Ww3YH/G9X5RbJJ7Lf/2AQQ2QfzpCMBmDpUXTF1QWYPdYtkmfK6e2I//PXNXCpLiUCklkimT7+87h59+Et1KBGIpEoFD2GdtmEb7ffs57CG9co/xhHQrMmhHNuw0cHbCsOfVKhFzxBI9SkRiEQo9lv+PVsmkLt3NWsatIm77zdhEji1xZGpDLFqjusMnQekOwqpIUoEIhEo786dNQ3aMOro35X7vn5dW3KqunckxZQIRKohmcHb0r573WkjtZQSgUg5NHgr2UKJQLLe5IK17Jj7F87eNeeA9W2K93EbFRQUq8zgrQZYpZZSIpA6pSplFQo+3cqUhrPJrbf2gIHcnEb1gw/5nDgf8lWhAVappZQIpE55bcnnpQXIktUjryl5uw/n8JzTObWWVNMUSSUlAsk4ib71V7kK5aQa+tYvkoE0eb1knJJv/fGoCqVI5alFIBmjpCVw0Lf+sk/sVuUJWw3kShZTIpCMMLlgLb+a9hHw3a2ZpWpi2kQN5EoWUyKQWq3sE7oPXNo5/r35tWjaRJFMo0QgtVK8Eg16QEskGkoEUruE/f2llTiPqPfdA1vl9f2rf1+kWpQIpFbZOPdZmny5nG/8xOQrcap/X6RalAikVtm8Yzdr/UQeavEHVeIUSRElAqkVSsYE7ghr8lf6gTARqTIlAkmLsk8HlwwKl4wJiEjqKBFkiwqmSawpG7cXs3nH7gr3O6iyZ0kVz51rIUcDvyKppESQLWrioatQog/77cX7gApKN5OgsmeOBn5FUk2JIJvU0ENXNz/xPoVby6/wqfv9RTKLEoFUqGx/fpUrfIpIraREkImq0t+fRLdQMvPvgip8itQ1SgSZqCr9/QkeuopXziGWyjuI1G1KBJmqkv39kwvW8tqiz2HR+wdtUz0fkeymRJBiVZlTt6x7tmwDYNQTB3+ol6e8b/sl65QARLKXEkGKVWVO3ZqgD3sRKY8SQYokPbtWMmwtHNeZqUN1146IVF+kcxabWR8zW2Fmq8xsRJztrc1sjpl9YGYfmlnfKONJp9gkEHd2rcpQtU0RqUGRtQjMrB4wDvghUAQsMLPp7h5bUX4k8IK7/8nMOgIzgNyoYkqXyQVrKfh0Kz3ymsa/916za4lIGkXZIugOrHL31e6+B5gC9CuzjwMlneVHAusjjCctYufa1b33IlIbRZkIWgLrYpaLwnWx7gOGmFkRQWvgpngHMrNhZrbQzBZu2rQpilgjEZsEyp1rV0QkzaIcLLY467zM8lXAU+7+ezPrBTxjZp3c/dsD3uQ+HhgPkJ+fX/YYtU7sA1pX1XuLW49dyrGFjTTNoojUSlG2CIqAE2KWW3Fw18+1wAsA7v4+0AhoFmFMKVEyMNwjr2mQBHauLH9nDfyKSJpF2SJYALQ3szzgc2AQcHWZfdYC5wFPmVkHgkSQOX0/oYRF2SY1CkorazBYRGqpyFoE7r4PGA7MApYT3B20zMxGmdkl4W6/AK43s6XA88A17l7ru35ilYwDlDy5CyrKJiKZJdIHytx9BsEgcOy6e2JeFwJnRxlDVMoWaisdDC55QKyQ4EdjACJSy+nJ4iqKHQc4oHRD2cqgGgMQkVpOiaCSyi0VEUsPiIlIBlEiqITY5wJKWgIiIplOiaASSu4M0sNhIlKXKBGUkWi+gJIxASUBEalLIq0+molK+v/j0W2hIlIXqUUQR7mDwCIidZBaBCIiWS6rWwTxxgMOmEayKrOH6QEyEckwSbUIzKyhmbWLOphUizcecMA4gGYPE5EsUGGLwMwuAv4ANATyzKwrcK+7Xxp1cKlQ4XiAHg4TkToumRbBKKAH8BWAuy8B6lzrQEQkWyWTCPa6+1dl1mVUhVARESlfMoPFy83sSuCQcG6BW4B50YYVrbL1gkREslkyLYLhwBnAt8ArQDFBMshIsfMH6AExEZHkWgQXuPvdwN0lK8zsMoKkkHFUL0hE5EDJtAhGxln365oOJJVUL0hE5DvltgjM7AKgD9DSzP4Qs+kIgm4iERGpAxJ1DX0BfEwwJrAsZv12YESUQaVMRU8O6ylhEckC5SYCd/8A+MDMnnP34hTGFJnJBWsp+HQrPfKaBivKTitZlp4SFpEskMxgcUsz+0+gI9CoZKW7nxRZVBEpGSg+4E4hPTksIlkumcHip4BJgAEXAi8AUyKMKVIaKBYROVAyieAwd58F4O7/cveRwA+iDUtERFIlma6h3WZmwL/M7Abgc+CYaMMSEZFUSSYR3AY0AW4G/hM4EvhplEGJiEjqVJgI3L0gfLkd+BGAmbWKMigREUmdhGMEZnammfU3s2bh8qlm9lcyvOiciIh8p9xEYGb/D3gOGAz83cx+DcwBlgIZd+uoiIjEl6hrqB9wmrvvMrOmwPpweUVqQhMRkVRI1DVU7O67ANx9K/A/SgIiInVPohZBGzMrKTVtQG7MMu5+WUUHN7M+wCNAPeBJdx8dZ58rgfsIZj1b6u5XJx++iIhUV6JEcHmZ5ccrc2AzqweMA34IFAELzGy6uxfG7NMe+CVwtrt/aWZ6PkFEJMUSFZ17q5rH7g6scvfVAGY2hWDcoTBmn+uBce7+ZXjOL6p5ThERqaRkSkxUVUtgXcxyUbgu1knASWb2npnNC7uSDmJmw8xsoZkt3LRpU0ThiohkpygTgcVZ52WW6wPtge8DVwFPmtlRB73Jfby757t7fvPmzWs8UBGRbJZ0IjCzQyt57CLghJjlVgS3oJbd5zV33+vunwIrCBKDiIikSIUlJsysOzCBoMZQazM7DbjO3W+q4K0LgPZmlkdQqG4QUPaOoFcJWgJPhU8vnwSsrtwlJGnhJO7ZMiF4PenI4E/NQCYiklSL4FHgP4AtAO6+lCTKULv7PmA4MAtYDrzg7svMbJSZXRLuNgvYYmaFBE8t3+nuWyp/GUn46CVy95bJMZqBTEQkqeqjh7j7Z0El6lL7kzm4u88AZpRZd0/MawduD38it6ZBG0Yd/TumDu2VitOJiGSEZBLBurB7yMNnA24CPok2LBERSZVkuoZuJPjG3hrYCPQM14mISB2QTItgn7sPijwSERFJi2RaBAvMbIaZ/cTMciKPKCIbtxezvXhfusMQEal1KkwE7t4W+C1wBvCRmb1qZhnXQti8YzcA/bqWfbhZRCS7JfVAmbvPdfebgW7A1wQT1mScnEb1ubpH63SHISJSq1SYCMysiZkNNrP/AuYDm4CzIo9MRERSIpnB4o+B/wLGuPu7EccjIiIplkwiaOPu30YeiYiIpEW5icDMfu/uvwBeNrOyVUOTmqFMRERqv0Qtgqnhn5WamUxERDJLohnK5ocvO7j7AcnAzIYD1Z3BTEREaoFkbh/9aZx119Z0ICIikh6JxggGEswhkGdmr8RsygG+ijowERFJjURjBPMJ5iBoBYyLWb8d+CDKoEREJHUSjRF8CnwKvJm6cEREJNUSdQ294+7nmtmXHDjpvBHMKdM08uhERCRyibqGSqajbJaKQEREJD3KvWso5mniE4B67r4f6AX8DDg8BbGJiEgKJHP76KsE01S2Bf4KdAAmRxqViIikTDKJ4Ft33wtcBjzs7jcBKuovIlJHJJMI9pnZFcCPgNfDdQ2iC0lERFIp2SeLf0BQhnq1meUBz0cbloiIpEqFZajd/WMzuxloZ2anAKvc/T+jD01ERFKhwkRgZv8beAb4nOAZguPM7Efu/l7UwYmISPSSmZhmLNDX3QsBzKwDQWLIjzIwERFJjWTGCBqWJAEAd18ONIwuJBERSaVkWgSLzewJglYAwGBUdE5EpM5IJhHcANwM3EUwRvAP4LEogxIRkdRJmAjMrDPQFpjm7mNSE5KIiKRSuWMEZvYrgvISg4HZZhZvpjIREclwiQaLBwNd3P0K4Ezgxsoe3Mz6mNkKM1tlZiMS7DfAzNzMdCeSiEiKJUoEu919J4C7b6pg34OYWT2Cmc0uBDoCV5lZxzj75RCMQRRU5vgiIlIzEo0RtImZq9iAtrFzF7v7ZRUcuzvBU8irAcxsCtAPKCyz3/3AGOCOygQuIiI1I1EiuLzM8uOVPHZLYF3MchHQI3YHMzsdOMHdXzezchOBmQ0DhgG0bt26kmGIiEgiieYsfquax7Z4hy3daHYIwVPL11R0IHcfD4wHyM/P9wp2FxGRSqhUv38lFRHMblaiFbA+ZjkH6AT8t5mtAXoC0zVgLCKSWlEmggVAezPLM7OGwCBgeslGd9/m7s3cPdfdc4F5wCXuvjDCmEREpIykE4GZHVqZA7v7PmA4MAtYDrzg7svMbJSZXVK5MEVEJCrJlKHuDkwAjgRam9lpwHXhlJUJufsMYEaZdfeUs+/3kwlYRERqVjItgkeB/wC2ALj7UoIZy0REpA5IJhEc4u6flVm3P4pgREQk9ZKpProu7B7y8Gnhm4BPog1LRERSJZkWwY3A7UBrYCPBbZ6VrjskIiK1UzKT139BcOuniIjUQcncNfQXYp4ILuHuwyKJSEREUiqZMYI3Y143Ai7lwBpCIiKSwZLpGpoau2xmzwCzI4tIRERSqiolJvKAE2s6EBERSY9kxgi+5LsxgkOArUC5s42JiEhmqWjyegNOAz4PV33r7ioDLSJShyTsGgo/9Ke5+/7wR0lARKSOSWaMYL6ZdYs8EhERSYtyu4bMrH5YSvoc4Hoz+xewk2DmMXd3JQcRkTog0RjBfKAb0D9FsYiISBokSgQG4O7/SlEsIiKSBokSQXMzu728je7+hwjiERGRFEuUCOoBTQhbBiIiUjclSgQb3H1UyiIREZG0SHT7qFoCIiJZIFEiOC9lUYiISNqUmwjcfWsqAxERkfSoSvVRERGpQ5QIRESynBKBiEiWUyIQEclySgQiIllOiUBEJMspEYiIZDklAhGRLBdpIjCzPma2wsxWmdlBE96b2e1mVmhmH5rZW2Z2YpTxiIjIwSJLBGZWDxgHXAh0BK4ys45ldvsAyHf3LsBLwJio4hERkfiibBF0B1a5+2p33wNMAfrF7uDuc9z9m3BxHtAqwnhERCSOKBNBS2BdzHJRuK481wIz420ws2FmttDMFm7atKkGQxQRkSgTQbwy1h53R7MhQD7wu3jb3X28u+e7e37z5s1rMEQREUk0MU11FQEnxCy3AtaX3cnMzgd+DZzr7rsjjEdEROKIskWwAGhvZnlm1hAYBEyP3cHMTgeeAC5x9y8ijEVERMoRWSJw933AcGAWsBx4wd2XmdkoM7sk3O13BPMiv2hmS8xsejmHExGRiETZNYS7zwBmlFl3T8zr86M8v4iIVExPFouIZDklAhGRLKdEICKS5ZQIRESynBKBiEiWUyIQEclykd4+KiK13969eykqKqK4uDjdoUgNaNSoEa1ataJBgwZJv0eJQCTLFRUVkZOTQ25uLmbxSoRJpnB3tmzZQlFREXl5eUm/T11DIlmuuLiYo48+WkmgDjAzjj766Eq37pQIRERJoA6pyt+lEoGISJZTIhCRtNqyZQtdu3ala9euHHfccbRs2bJ0ec+ePUkdY+jQoaxYsSLpcz755JM0b96crl27csopp/Doo48esP1Pf/oTp5xyCqeccgo9evRg7ty5pdv27t3LXXfdRbt27ejUqRM9evRg1qxZSZ+7NtJgsYik1dFHH82SJUsAuO+++2jSpAl33HHHAfu4O+7OIYfE/+46adKkSp938ODBPPzww2zatImTTz6ZK664ghYtWvDqq68yadIk5s6dS9OmTVm4cCGXXXYZixYtonnz5vzyl79k69atFBYW0rBhQzZs2MB7771X+QtPYP/+/dSrV69Gj5mIEoGIlPrNfy2jcP3XNXrMjscfwb0Xn1rp961atYr+/ftzzjnnUFBQwOuvv85vfvMbFi9ezK5duxg4cCD33BMUMz7nnHN4/PHH6dSpE82aNeOGG25g5syZHHbYYbz22mscc8wx5Z6nefPmtGnThg0bNtCiRQsefPBBHnroIZo2bQpAfn4+gwcP5o9//CO33347Tz31FGvWrKFhw4YAtGjRggEDBhx03IKCAm699Va++eYbGjVqxJw5c5g8eTIff/wxDz/8MAB9+vRh5MiR9OzZk2bNmjF8+HDeeOMNLr74YpYvX87kyZMBePPNNxk3bhzTpk1j5syZjBo1it27d9O+fXsmTpzI4YcfXunfbyx1DYlIrVVYWMi1117LBx98QMuWLRk9ejQLFy5k6dKlzJ49m8LCwoPes23bNs4991yWLl1Kr169mDhxYsJzrFmzhv3799OpU6fSc55xxhkH7JOfn8+yZctYuXIleXl5NGnSJOExi4uLGTRoEOPGjWPp0qW88cYbHHrooQnfs23bNrp168b8+fO5++67effdd9m1axcAU6dOZeDAgXzxxReMHj2at956i8WLF9OlSxceeeSRhMdNhloEIlKqKt/co9S2bVvOPPPM0uXnn3+eCRMmsG/fPtavX09hYSEdO3Y84D2NGzfmwgsvBOCMM87g3XffjXvs5557jtmzZ7NixQomTZpU+g0/Hnev1N04y5cvp3Xr1nTr1g2AI488ssL3NGzYkEsvvbT09Q9/+EP+9re/0a9fP2bNmsXDDz9cmvzOOussAPbs2cM555yTdFzlUSIQkVortstj5cqVPPLII8yfP5+jjjqKIUOGxL1fPvYDvV69euzbty/usUvGCP75z39yySWXcMEFF3DMMcfQoUMHFi1axPe+973SfRcvXkzHjh1p3749n376KTt37kzYHVNe4qhfvz7ffvtt6XJs/I0bNz7gPQMHDmTChAkcdthh9OrVi8MPPxx3p0+fPjzzzDPlnrsq1DUkIhnh66+/JicnhyOOOIINGzbU2J0655xzDldddRWPPfYYAHfddRd33XUXX375JRAkgWeffZYbb7yRnJwcfvzjH3Prrbeyd+9eANavX89zzz13wDFPPfVUPvvsMxYvXlwa+/79+8nNzeWDDz7A3VmzZg2LFi0qN67zzjuPgoICJkyYwMCBAwE466yzeOedd1i9ejUAO3fuZOXKldX+HahFICIZoVu3bnTs2JFOnTrRpk0bzj777Bo79ogRI+jevTsjRozgsssuY8OGDfTs2RMz44gjjmDy5MmlA86jR4/mV7/6FR06dKBx48Ycfvjh3H///Qcc79BDD+X555/nxhtvpLi4mMaNG/P2229z7rnn0rJlSzp37kynTp3o2rVruTHVr1+fCy+8kMmTJ5cmmmOPPbY0MZTcWvvAAw/Qvn37al2/uXu1DpBq+fn5vnDhwkq/b9kDQT/aqb/6Z02HJJLRli9fTocOHdIdhtSgeH+nZrbI3fPj7a+uIRGRLKdEICKS5ZQIRESynBKBiEiWUyIQEclySgQiIllOiUBE0qomylADTJw4kX//+99xtw0ZMoS8vDy6du3Kaaedxpw5c0q37d69m5tuuom2bdvSvn17+vfvz/r160u3r1+/niuvvJJ27drRsWNHLrroIlatWlX1C66FlAhEJK1KylAvWbKEG264gdtuu610OVH9n7ISJQKAsWPHsmTJEh566CF+/vOfl66/++672b17N5988gkrV67koosu4vLLLweCUhH9+/end+/erFq1isLCQu6//342btxY9Qsuo7wSGKmkJ4tF5DszR8C/P6rZYx7XGS4cXaW3Pv3004wbN449e/Zw1lln8fjjj/Ptt98ydOhQlixZgrszbNgwjj32WJYsWcLAgQNp3Lgx8+fPLzeJ9OrVi88//xyA7du38+yzz7JmzZrS+v/XX389EydO5J133mH37t00adKE6667rvT9JYXkypo0aRJjx47FzOjWrRuTJk1iyJAhDBgwgP79+wPQpEkTduzYwZtvvsno0aNp1qwZy5Yto3fv3px88skMGzYMgJEjR9K8eXNuueUWRo8ezSuvvEJxcTEDBgwoLb1dk5QIRKRW+vjjj5k2bRpz586lfv36DBs2jClTptC2bVs2b97MRx8FCeurr77iqKOO4rHHHuPxxx9PWLYB4O9//3vpB3N5ZaVLyk4XFxcfVJI6nqVLl/Lggw+WTmazdevWCt8zb948CgsLad26NQsWLGDEiBGlieDFF19kzpw5zJgxg7Vr11JQUIC707dvX+bOnVtafbSmKBGIyHeq+M09Cm+++SYLFiwgPz+oirBr1y5OOOEELrjgAlasWMEtt9xC37596d27d1LHu+2227jtttvYvHkz8+fPB8qvElrZstNvv/02AwcOLJ3MpuTPRHr16kXr1q0BOPPMM1m3bh0bN26kqKiI4447juOPP54xY8Ywc+ZMTj/9dAB27NjBJ598klmJwMz6AI8A9YAn3X10me2HAn8FzgC2AAPdfU2UMYlIZnB3fvrTnx5U0A3gww8/ZObMmTz66KO8/PLLjB8/vsLjjR07losvvpixY8dyzTXXUFBQwEknncTq1avZsWPHAa2CxYsXc8UVV1BcXMzrr7+eVKwVlZ3ev3//AeMBZctYX3755bz88susWbOGQYMGlR535MiRXHvttRXGUB2RDRabWT1gHHAh0BG4ysw6ltntWuBLd28HjAUejCoeEcks559/Pi+88AKbN28GgruL1q5dy6ZNm3B3rrjiitKpKwFycnLYvn17wmPWq1ePX/ziF3zzzTe89dZb5OTkcPXVV3PnnXeWfmBPnDiR/fv3c+6559K7d2++/vrrA2Y5KygoOGiym/PPP58pU6aUdgmV/Jmbm1taanratGns37+/3NgGDRrElClTeOWVV0oHqy+44AImTJjAzp07ASgqKir9fdSkKO8a6g6scvfV7r4HmAL0K7NPP+Dp8PVLwHlWmfaYiNRZnTt35t577+X888+nS5cu9O7dm40bN7Ju3Tq+973v0bVrV66//noeeOABAIYOHcp1111X4W2nZsbIkSMZM2YMAGPGjOGQQw6hffv2tGvXjldffZVXXnmldN/XXnuNGTNm0LZtWzp16sRvf/tbjj/++AOO2aVLF+66667SuO68804AfvaznzF79my6d+/OkiVLEk5Xedppp7Fp0yby8vJKS1737duXAQMG0LNnTzp37syVV17Jjh07qv5LLe93ElUZajMbAPRx9+vC5R8BPdx9eMw+H4f7FIXL/wr32VzmWMOAYQCtW7c+47PPPqt0PPP+eD0APX/+lypdj0hdpTLUdU9ly1BHOUYQ75t92ayTzD64+3hgPATzEVQlGCUAEZH4ouwaKgJOiFluBawvbx8zqw8cCVR835WIiNSYKBPBAqC9meWZWUNgEDC9zD7TgZ+ErwcAb3umTZkmUgfov13dUZW/y8gSgbvvA4YDs4DlwAvuvszMRpnZJeFuE4CjzWwVcDswIqp4RCS+Ro0asWXLFiWDOsDd2bJlC40aNarU+7JmzmIRiW/v3r0UFRVRXFyc7lCkBjRq1IhWrVrRoEGDA9ana7BYRDJAgwYNyMvLS3cYkkaqPioikuWUCEREspwSgYhIlsu4wWIz2wRU/tHiQDOg5gt11G665uyga84O1bnmE929ebwNGZcIqsPMFpY3al5X6Zqzg645O0R1zeoaEhHJckoEIiJZLtsSQcWzV9Q9uubsoGvODpFcc1aNEYiIyMGyrUUgIiJlKBGIiGS5OpkIzKyPma0ws1VmdlBFUzM71MymhtsLzCw39VHWrCSu+XYzKzSzD83sLTM7MR1x1qSKrjlmvwFm5maW8bcaJnPNZnZl+He9zMwmpzrGmpbEv+3WZjbHzD4I/333TUecNcXMJprZF+EMjvG2m5k9Gv4+PjSzbtU+qbvXqR+gHvAvoA3QEFgKdCyzz8+BP4evBwFT0x13Cq75B8Bh4esbs+Gaw/1ygH8A84D8dMedgr/n9sAHwP8Kl49Jd9wpuObxwI3h647AmnTHXc1r/h7QDfi4nO19gZkEMzz2BAqqe8662CLoDqxy99XuvgeYAvQrs08/4Onw9UvAeWYWb9rMTFHhNbv7HHf/JlycRzBjXCZL5u8Z4H5gDFAXaiwnc83XA+Pc/UsAd/8ixTHWtGSu2YEjwtdHcvBMiBnF3f9B4pka+wF/9cA84Cgza1Gdc9bFRNASWBezXBSui7uPBxPobAOOTkl00UjmmmNdS/CNIpNVeM1mdjpwgru/nsrAIpTM3/NJwElm9p6ZzTOzPimLLiyzCPQAAAWiSURBVBrJXPN9wBAzKwJmADelJrS0qez/9wrVxfkI4n2zL3uPbDL7ZJKkr8fMhgD5wLmRRhS9hNdsZocAY4FrUhVQCiTz91yfoHvo+wStvnfNrJO7fxVxbFFJ5pqvAp5y99+bWS/gmfCav40+vLSo8c+vutgiKAJOiFluxcFNxdJ9zKw+QXMyUVOstkvmmjGz84FfA5e4++4UxRaViq45B+gE/LeZrSHoS52e4QPGyf7bfs3d97r7p8AKgsSQqZK55muBFwDc/X2gEUFxtroqqf/vlVEXE8ECoL2Z5ZlZQ4LB4Oll9pkO/CR8PQB428NRmAxV4TWH3SRPECSBTO83hgqu2d23uXszd89191yCcZFL3D2T5zlN5t/2qwQ3BmBmzQi6ilanNMqalcw1rwXOAzCzDgSJYFNKo0yt6cCPw7uHegLb3H1DdQ5Y57qG3H2fmQ0HZhHccTDR3ZeZ2ShgobtPByYQNB9XEbQEBqUv4upL8pp/BzQBXgzHxde6+yVpC7qakrzmOiXJa54F9DazQmA/cKe7b0lf1NWT5DX/AviLmd1G0EVyTSZ/sTOz5wm69pqF4x73Ag0A3P3PBOMgfYFVwDfA0GqfM4N/XyIiUgPqYteQiIhUghKBiEiWUyIQEclySgQiIllOiUBEJMspEUitY2b7zWxJzE9ugn1zy6vSWMlz/ndY4XJpWJ7h5Coc4wYz+3H4+hozOz5m25Nm1rGG41xgZl2TeM+tZnZYdc8tdZcSgdRGu9y9a8zPmhSdd7C7n0ZQkPB3lX2zu//Z3f8aLl4DHB+z7Tp3L6yRKL+L848kF+etgBKBlEuJQDJC+M3/XTNbHP6cFWefU81sftiK+NDM2ofrh8Ssf8LM6lVwun8A7cL3nhfWuf8orBN/aLh+tH03v8ND4br7zOwOMxtAUM/pufCcjcNv8vlmdqOZjYmJ+Roze6yKcb5PTLExM/uTmS20YB6C34TrbiZISHPMbE64rreZvR/+Hl80syYVnEfqOCUCqY0ax3QLTQvXfQH80N27AQOBR+O87wbgEXfvSvBBXBSWHBgInB2u3w8MruD8FwMfmVkj4ClgoLt3JngS/0YzawpcCpzq7l2A38a+2d1fAhYSfHPv6u67Yja/BFwWszwQmFrFOPsQlJQo8Wt3zwe6AOeaWRd3f5SgDs0P3P0HYdmJkcD54e9yIXB7BeeROq7OlZiQOmFX+GEYqwHweNgnvp+ghk5Z7wO/NrNWwCvuvtLMzgPOABaEpTUaEySVeJ4zs13AGoJSxicDn7r7J+H2p4H/CzxOML/Bk2b2NyDpMtfuvsnMVoc1YlaG53gvPG5l4jycoORC7OxUV5rZMIL/1y0IJmn5sMx7e4br3wvP05Dg9yZZTIlAMsVtwEbgNIKW7EETzbj7ZDMrAC4CZpnZdQQle592918mcY7BsUXpzCzuHBVh/ZvuBIXOBgHDgf9TiWuZClwJ/A8wzd3dgk/lpOMkmKlrNDAOuMzM8oA7gDPd/Usze4qg+FpZBsx296sqEa/UceoakkxxJLAhrDH/I4JvwwcwszbA6rA7ZDpBF8lbwAAzOybcp6klP1/z/wC5ZtYuXP4R8E7Yp36ku88gGIiNd+fOdoJS2PG8AvQnqKM/NVxXqTjdfS9BF0/PsFvpCGAnsM3MjgUuLCeWecDZJddkZoeZWbzWlWQRJQLJFH8EfmJm8wi6hXbG2Wcg8LGZLQFOIZjOr5DgA/MNM/sQmE3QbVIhdy8mqOz4opl9BHwL/JngQ/X18HjvELRWynoK+HPJYHGZ434JFAInuvv8cF2l4wzHHn4P3OHuSwnmKl4GTCTobioxHphpZnPcfRPBHU3Ph+eZR/C7kiym6qMiIllOLQIRkSynRCAikuWUCEREspwSgYhIllMiEBHJckoEIiJZTolARCTL/X+/Bh1UrDltkQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cols = ['Train Accuracy', 'Train Precision', 'Train Recall', 'Train F-Score', 'Train Auc', 'Test Accuracy',  'Test Precision', 'Test Recall', 'Test F-Score', 'Test Auc']\n",
    "kmeans_stat = pd.DataFrame(columns=cols, index=range(30))\n",
    "for i in range(30):\n",
    "    kmeans_stat.loc[i] = k_means(x_train, y_train, x_test, y_test)\n",
    "print(\"K Means AVG Score:\\n\", kmeans_stat.mean())\n",
    "\n",
    "kmeans = KMeans(n_clusters=2, random_state=233).fit(x_train)\n",
    "train_labels = kmeans.labels_\n",
    "train_cluster_centers = kmeans.cluster_centers_\n",
    "# get two clusters center\n",
    "train_center_0 = train_cluster_centers[0]\n",
    "train_center_1 = train_cluster_centers[1]\n",
    "train_cluster_0 = np.where(train_labels==0)\n",
    "train_cluster_1 = np.where(train_labels==1)\n",
    "\n",
    "cluster_0 = get_label(x_train, y_train, train_cluster_0, train_center_0)\n",
    "cluster_1 = get_label(x_train, y_train, train_cluster_1, train_center_1)\n",
    "\n",
    "y_train_predict_labels = train_labels\n",
    "y_train_predict_labels[cluster_0] = cluster_0\n",
    "y_train_predict_labels[cluster_1] = cluster_1\n",
    "\n",
    "# get confusion matrix\n",
    "train_confusion_matrix = confusion_matrix(y_train, y_train_predict_labels)\n",
    "print('\\nTrain Confusion Matrix\\n', train_confusion_matrix)\n",
    "# test data\n",
    "test_labels = kmeans.predict(x_test)\n",
    "test_cluster_0 = np.where(test_labels==0)\n",
    "test_cluster_1 = np.where(test_labels==1)\n",
    "\n",
    "test_predict_labels = test_labels\n",
    "test_predict_labels[test_cluster_0] = cluster_0\n",
    "test_predict_labels[test_cluster_1] = cluster_1\n",
    "test_confusion_matrix = confusion_matrix(y_test, test_predict_labels)\n",
    "print('Test Confusion Matrix\\n', test_confusion_matrix)\n",
    "\n",
    "# draw roc for each one time\n",
    "train_fpr, train_tpr, train_thresholds = roc_curve(y_train, pd.DataFrame(kmeans.transform(x_train)).iloc[:,0])\n",
    "test_fpr, test_tpr, test_thresholds = roc_curve(y_test, pd.DataFrame(kmeans.transform(x_test)).iloc[:,0])\n",
    "plt.plot(train_fpr, train_tpr, label='Train ROC curve')\n",
    "plt.plot(test_fpr, test_tpr, label='Test ROC curve')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### iv. Spectral Clustering: Repeat 1(b)iii using spectral clustering, which is clus- tering based on kernels. Research what spectral clustering is. Use RBF kernel with gamma=1 or find a gamma for which the two clutsres have the same balance as the one in original data set (if the positive class has p and the negative class has n samples, the two clusters must have p and n members). Do not label data based on their proximity to cluster center, because spectral clustering may give you non-convex clusters . Instead, use fit − predict method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-20T01:34:46.546447Z",
     "start_time": "2019-11-20T01:34:46.543923Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import SpectralClustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-20T01:34:46.559138Z",
     "start_time": "2019-11-20T01:34:46.548937Z"
    }
   },
   "outputs": [],
   "source": [
    "def spectral_cluster(x_train, y_train, x_test, y_test):\n",
    "    normalized_x_train = pd.DataFrame(preprocessing.normalize(x_train))\n",
    "    normalized_x_test = pd.DataFrame(preprocessing.normalize(x_test))\n",
    "    \n",
    "    y_train = np.array(y_train)\n",
    "    y_test = np.array(y_test)\n",
    "    \n",
    "    spectral_clusters = SpectralClustering(n_clusters=2, gamma=1, affinity='rbf').fit(normalized_x_train)\n",
    "    train_labels = spectral_clusters.labels_\n",
    "    # get two clusters center\n",
    "    train_cluster_0 = np.where(train_labels==0)\n",
    "    train_cluster_1 = np.where(train_labels==1)\n",
    "\n",
    "    cluster_0 = np.argmax(np.bincount(y_train[train_cluster_0]))\n",
    "    cluster_1 = np.argmax(np.bincount(y_train[train_cluster_1]))\n",
    "\n",
    "    y_train_predict_labels = train_labels\n",
    "    y_train_predict_labels[cluster_0] = cluster_0\n",
    "    y_train_predict_labels[cluster_1] = cluster_1\n",
    "\n",
    "    train_accuracy = accuracy_score(y_train, y_train_predict_labels)\n",
    "\n",
    "    # get confusion matrix\n",
    "    train_confusion_matrix = confusion_matrix(y_train, y_train_predict_labels)\n",
    "\n",
    "    # test data\n",
    "    test_labels = spectral_clusters.fit_predict(normalized_x_test)\n",
    "    test_cluster_0 = np.where(test_labels==0)\n",
    "    test_cluster_1 = np.where(test_labels==1)\n",
    "    \n",
    "    test_predict_labels = test_labels\n",
    "    test_predict_labels[test_cluster_0] = cluster_0\n",
    "    test_predict_labels[test_cluster_1] = cluster_1\n",
    "    \n",
    "    test_accuracy = accuracy_score(y_test, test_predict_labels)\n",
    "    test_confusion_matrix = confusion_matrix(y_test, test_predict_labels)\n",
    "    # get parameter\n",
    "    train_tn, train_fp, train_fn, train_tp = train_confusion_matrix.ravel()\n",
    "    test_tn, test_fp, test_fn, test_tp = test_confusion_matrix.ravel()\n",
    "    # precision: p = tp/(tp+fp)\n",
    "    train_precision = train_tp/(train_tp + train_fp)    \n",
    "    test_precision = test_tp/(test_tp + test_fp)\n",
    "\n",
    "    # recall: tp/(tp+fn)\n",
    "    train_recall = train_tp/(train_tp + train_fn)\n",
    "    test_recall = test_tp/(test_tp + test_fn)\n",
    "    \n",
    "    # f1 = 2 * (P*R)/(P+R)\n",
    "    train_f1 = 2 * (train_precision * train_recall) / (train_precision + train_recall)\n",
    "    test_f1 = 2 * (test_precision * test_recall) / (test_precision + test_recall)\n",
    "    \n",
    "    return [train_accuracy, train_precision, train_recall, train_f1, test_accuracy, test_precision, test_recall, test_f1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-20T01:46:06.688287Z",
     "start_time": "2019-11-20T01:46:04.590174Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spectral Cluster AVG Score:\n",
      " Train Accuracy     0.628488\n",
      "Train Precision    0.699752\n",
      "Train Recall       0.570020\n",
      "Train F-Score      0.613279\n",
      "Test Accuracy      0.455362\n",
      "Test Precision     0.441820\n",
      "Test Recall        0.470543\n",
      "Test F-Score       0.445021\n",
      "dtype: float64\n",
      "\n",
      "Train Confusion Matrix\n",
      " [[282   3]\n",
      " [ 49 120]]\n",
      "Test Confusion Matrix\n",
      " [[65  7]\n",
      " [12 31]]\n"
     ]
    }
   ],
   "source": [
    "cols = ['Train Accuracy', 'Train Precision', 'Train Recall', 'Train F-Score', 'Test Accuracy',  'Test Precision', 'Test Recall', 'Test F-Score']\n",
    "spectral_stat = pd.DataFrame(columns=cols, index=range(30))\n",
    "for i in range(30):\n",
    "    spectral_stat.loc[i] = spectral_cluster(x_train, y_train, x_test, y_test)\n",
    "print(\"Spectral Cluster AVG Score:\\n\", spectral_stat.mean())\n",
    "\n",
    "normalized_x_train = pd.DataFrame(preprocessing.normalize(x_train))\n",
    "normalized_x_test = pd.DataFrame(preprocessing.normalize(x_test))\n",
    "y_s_train = np.array(y_train)\n",
    "y_s_test = np.array(y_test)\n",
    "spectral_clusters = SpectralClustering(n_clusters=2, gamma=1, affinity='rbf').fit(normalized_x_train)\n",
    "train_labels = spectral_clusters.labels_\n",
    "# get two clusters center\n",
    "train_cluster_0 = np.where(train_labels==0)\n",
    "train_cluster_1 = np.where(train_labels==1)\n",
    "\n",
    "cluster_0 = np.argmax(np.bincount(y_s_train[train_cluster_0]))\n",
    "cluster_1 = np.argmax(np.bincount(y_s_train[train_cluster_1]))\n",
    "\n",
    "y_train_predict_labels = train_labels\n",
    "y_train_predict_labels[cluster_0] = cluster_0\n",
    "y_train_predict_labels[cluster_1] = cluster_1\n",
    "\n",
    "# get confusion matrix\n",
    "train_confusion_matrix = confusion_matrix(y_s_train, y_train_predict_labels)\n",
    "print('\\nTrain Confusion Matrix\\n', train_confusion_matrix)\n",
    "# test data\n",
    "test_labels = spectral_clusters.fit_predict(normalized_x_test)\n",
    "test_cluster_0 = np.where(test_labels==0)\n",
    "test_cluster_1 = np.where(test_labels==1)\n",
    "\n",
    "test_predict_labels = test_labels\n",
    "test_predict_labels[test_cluster_0] = cluster_0\n",
    "test_predict_labels[test_cluster_1] = cluster_1\n",
    "\n",
    "test_confusion_matrix = confusion_matrix(y_s_test, test_predict_labels)\n",
    "print('Test Confusion Matrix\\n', test_confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### v. One can expect that supervised learning on the full data set works better than semi-supervised learning with half of the data set labeled.One can expect that unsupervised learning underperforms in such situations. Compare the results you obtained by those methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-20T01:46:29.582425Z",
     "start_time": "2019-11-20T01:46:29.569248Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>supervised</th>\n",
       "      <th>semi_supervised</th>\n",
       "      <th>k_means</th>\n",
       "      <th>spectral_unsupervised</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Test Accuracy</td>\n",
       "      <td>0.953333</td>\n",
       "      <td>0.947826</td>\n",
       "      <td>0.826087</td>\n",
       "      <td>0.455362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Test Auc</td>\n",
       "      <td>0.964966</td>\n",
       "      <td>0.956632</td>\n",
       "      <td>0.906977</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Test F-Score</td>\n",
       "      <td>0.935680</td>\n",
       "      <td>0.926829</td>\n",
       "      <td>0.696970</td>\n",
       "      <td>0.445021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Test Precision</td>\n",
       "      <td>0.964209</td>\n",
       "      <td>0.974359</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.441820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Test Recall</td>\n",
       "      <td>0.909302</td>\n",
       "      <td>0.883721</td>\n",
       "      <td>0.534884</td>\n",
       "      <td>0.470543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Train Accuracy</td>\n",
       "      <td>0.991630</td>\n",
       "      <td>0.991189</td>\n",
       "      <td>0.852423</td>\n",
       "      <td>0.628488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Train Auc</td>\n",
       "      <td>0.999738</td>\n",
       "      <td>0.999834</td>\n",
       "      <td>0.913028</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Train F-Score</td>\n",
       "      <td>0.988666</td>\n",
       "      <td>0.987999</td>\n",
       "      <td>0.754579</td>\n",
       "      <td>0.613279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Train Precision</td>\n",
       "      <td>0.994378</td>\n",
       "      <td>0.998039</td>\n",
       "      <td>0.990385</td>\n",
       "      <td>0.699752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Train Recall</td>\n",
       "      <td>0.983037</td>\n",
       "      <td>0.978198</td>\n",
       "      <td>0.609467</td>\n",
       "      <td>0.570020</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 supervised  semi_supervised   k_means  spectral_unsupervised\n",
       "Test Accuracy      0.953333         0.947826  0.826087               0.455362\n",
       "Test Auc           0.964966         0.956632  0.906977                    NaN\n",
       "Test F-Score       0.935680         0.926829  0.696970               0.445021\n",
       "Test Precision     0.964209         0.974359  1.000000               0.441820\n",
       "Test Recall        0.909302         0.883721  0.534884               0.470543\n",
       "Train Accuracy     0.991630         0.991189  0.852423               0.628488\n",
       "Train Auc          0.999738         0.999834  0.913028                    NaN\n",
       "Train F-Score      0.988666         0.987999  0.754579               0.613279\n",
       "Train Precision    0.994378         0.998039  0.990385               0.699752\n",
       "Train Recall       0.983037         0.978198  0.609467               0.570020"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame({'supervised':supervised_stat.mean(), \n",
    "                              'semi_supervised':semi_stat.mean(),\n",
    "                              'k_means':kmeans_stat.mean(), \n",
    "                              'spectral_unsupervised':spectral_stat.mean()})\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Active Learning Using Support Vector Machines\n",
    "#### (a) Choose 472 data points ran- domly as the test set, and the remaining 900 points as the training set. This is a binary classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-20T01:47:53.332191Z",
     "start_time": "2019-11-20T01:47:53.322946Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data_banknote_authentication.txt', header=None)\n",
    "x_data = df.iloc[:, 0:4]\n",
    "y_data = df[4]\n",
    "train_x, test_x, train_y, test_y = train_test_split(x_data, y_data, test_size=472/(900+472))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Repeat each of the following two procedures 50 times. You will have 50 errors for 90 SVMs per each procedure.\n",
    "#### i. Train a SVM with a pool of 10 randomly selected data points from the training set using linear kernel and L1 penalty. Select the penalty parameter using 10-fold cross validation. Repeat this process by adding 10 other randomly selected data points to the pool, until you use all the 900 points. Do NOT replace the samples back into the training set at each step. Calculate the test error for each SVM. You will have 90 SVMs that were trained using 10, 20, 30, ... , 900 data points and their 90 test errors. You have implemented passive learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-20T03:09:49.910284Z",
     "start_time": "2019-11-20T03:09:49.903698Z"
    }
   },
   "outputs": [],
   "source": [
    "def select_data(train_x, train_y):\n",
    "    x_select = pd.DataFrame()\n",
    "    y_select = pd.DataFrame()\n",
    "    # make sure two kind of classes are included\n",
    "    while len(y_select) < 1 or len(np.bincount(y_select)) < 2 or np.bincount(y_select).min() < 2:\n",
    "        # random select 10\n",
    "        random_indexes = np.ramdom.randint(0, len(train_x), 10)\n",
    "        x_select = train_x.iloc[random_indexes]\n",
    "        y_select = train_y.iloc[random_indexes]\n",
    "    # the remian x\n",
    "    remain_x_data = train_x.drop(train_x.index[random_indexes])\n",
    "    remain_y_data = train_y.drop(train_x.index[random_indexes])\n",
    "    \n",
    "    # update index\n",
    "    x_select = x_select.reset_index(drop=True)\n",
    "    y_select = y_select.reset_index(drop=True)\n",
    "    remain_x_data = remain_x_data.reset_index(drop=True)\n",
    "    remain_y_data = remain_y_data.reset_index(drop=True)\n",
    "    return x_select, y_select, remain_x_data, remain_y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-20T03:09:50.385229Z",
     "start_time": "2019-11-20T03:09:50.382667Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-20T03:21:55.416804Z",
     "start_time": "2019-11-20T03:21:55.403247Z"
    }
   },
   "outputs": [],
   "source": [
    "def passive_learning(train_x, train_y, test_x, test_y):\n",
    "    x_select = pd.DataFrame()\n",
    "    y_select = pd.DataFrame()\n",
    "    # make sure two kind of classes are included\n",
    "    while len(y_select) < 1 or len(np.bincount(y_select)) < 2 or np.bincount(y_select).min() < 2:\n",
    "        # random select 10\n",
    "        random_indexes = np.random.randint(0, len(train_x), 10)\n",
    "        x_select = train_x.iloc[random_indexes]\n",
    "        y_select = train_y.iloc[random_indexes]\n",
    "    # the remian x\n",
    "    remain_x_data = train_x.drop(train_x.index[random_indexes])\n",
    "    remain_y_data = train_y.drop(train_x.index[random_indexes])\n",
    "    \n",
    "    # update index\n",
    "    x_select = x_select.reset_index(drop=True)\n",
    "    y_select = y_select.reset_index(drop=True)\n",
    "    remain_x_data = remain_x_data.reset_index(drop=True)\n",
    "    remain_y_data = remain_y_data.reset_index(drop=True)\n",
    "    \n",
    "    \n",
    "#     x_select, y_select, remain_x_data, remain_y_data = select_data(train_x, train_y)\n",
    "    \n",
    "    errors = list()\n",
    "    for i in range(90):\n",
    "        if i < 2:\n",
    "            CV = 5\n",
    "        else:\n",
    "            CV = 10\n",
    "        parameters = {'C':np.power(10, np.arange(-3, 7,0.5))}\n",
    "        svc = LinearSVC(penalty='l1', dual=False)\n",
    "        clf = GridSearchCV(svc, parameters, cv=CV)\n",
    "        clf.fit(x_select, y_select)\n",
    "        best_C = clf.best_params_['C']\n",
    "        \n",
    "        # Refit the model with the best params\n",
    "        svc = LinearSVC(penalty='l1', dual=False, C=best_C)\n",
    "        svc.fit(x_select, y_select)\n",
    "        errors.append(1-svc.score(test_x, test_y))\n",
    "        \n",
    "        if i != 0:\n",
    "            # select 10 data from remain dataset\n",
    "            new_x = remain_x_data.sample(n=10, replace=False, random_state=None, axis=0)\n",
    "            random_indexes = np.array(new_x.index)\n",
    "#             print(random_indexes)\n",
    "#             print(len(remain_x_data), '\\n')\n",
    "            # append data to train set\n",
    "            x_select = pd.concat([x_select, remain_x_data.iloc[random_indexes]],axis=0, ignore_index=True)\n",
    "            y_select = pd.concat([y_select, remain_y_data.iloc[random_indexes]],axis=0, ignore_index=True)\n",
    "            # delete from remain data\n",
    "            remain_x_data = remain_x_data.drop(remain_x_data.index[random_indexes])\n",
    "            remain_y_data = remain_y_data.drop(remain_y_data.index[random_indexes])\n",
    "\n",
    "            #reset index\n",
    "            remain_x_data = remain_x_data.reset_index(drop=True)\n",
    "            remain_y_data = remain_y_data.reset_index(drop=True)\n",
    "    return errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-20T05:07:23.543361Z",
     "start_time": "2019-11-20T03:21:56.064187Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1  time\n",
      "2  time\n",
      "3  time\n",
      "4  time\n",
      "5  time\n",
      "6  time\n",
      "7  time\n",
      "8  time\n",
      "9  time\n",
      "10  time\n",
      "11  time\n",
      "12  time\n",
      "13  time\n",
      "14  time\n",
      "15  time\n",
      "16  time\n",
      "17  time\n",
      "18  time\n",
      "19  time\n",
      "20  time\n",
      "21  time\n",
      "22  time\n",
      "23  time\n",
      "24  time\n",
      "25  time\n",
      "26  time\n",
      "27  time\n",
      "28  time\n",
      "29  time\n",
      "30  time\n",
      "31  time\n",
      "32  time\n",
      "33  time\n",
      "34  time\n",
      "35  time\n",
      "36  time\n",
      "37  time\n",
      "38  time\n",
      "39  time\n",
      "40  time\n",
      "41  time\n",
      "42  time\n",
      "43  time\n",
      "44  time\n",
      "45  time\n",
      "46  time\n",
      "47  time\n",
      "48  time\n",
      "49  time\n",
      "50  time\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.228814</td>\n",
       "      <td>0.091102</td>\n",
       "      <td>0.150424</td>\n",
       "      <td>0.173729</td>\n",
       "      <td>0.158898</td>\n",
       "      <td>0.222458</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.190678</td>\n",
       "      <td>0.262712</td>\n",
       "      <td>0.156780</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.057203</td>\n",
       "      <td>0.046610</td>\n",
       "      <td>0.161017</td>\n",
       "      <td>0.273305</td>\n",
       "      <td>0.173729</td>\n",
       "      <td>0.069915</td>\n",
       "      <td>0.135593</td>\n",
       "      <td>0.197034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.228814</td>\n",
       "      <td>0.091102</td>\n",
       "      <td>0.150424</td>\n",
       "      <td>0.137712</td>\n",
       "      <td>0.158898</td>\n",
       "      <td>0.222458</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.190678</td>\n",
       "      <td>0.262712</td>\n",
       "      <td>0.156780</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.057203</td>\n",
       "      <td>0.046610</td>\n",
       "      <td>0.161017</td>\n",
       "      <td>0.273305</td>\n",
       "      <td>0.173729</td>\n",
       "      <td>0.069915</td>\n",
       "      <td>0.135593</td>\n",
       "      <td>0.190678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.114407</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.171610</td>\n",
       "      <td>0.067797</td>\n",
       "      <td>0.040254</td>\n",
       "      <td>0.186441</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.150424</td>\n",
       "      <td>0.038136</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.040254</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.052966</td>\n",
       "      <td>0.116525</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.042373</td>\n",
       "      <td>0.052966</td>\n",
       "      <td>0.078390</td>\n",
       "      <td>0.038136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.127119</td>\n",
       "      <td>0.095339</td>\n",
       "      <td>0.040254</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.038136</td>\n",
       "      <td>0.038136</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>...</td>\n",
       "      <td>0.048729</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.072034</td>\n",
       "      <td>0.038136</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.082627</td>\n",
       "      <td>0.082627</td>\n",
       "      <td>0.036017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.048729</td>\n",
       "      <td>0.095339</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.059322</td>\n",
       "      <td>0.036017</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>...</td>\n",
       "      <td>0.046610</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.091102</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.042373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6   \\\n",
       "0   0.228814  0.091102  0.150424  0.173729  0.158898  0.222458  0.023305   \n",
       "1   0.228814  0.091102  0.150424  0.137712  0.158898  0.222458  0.023305   \n",
       "2   0.114407  0.033898  0.171610  0.067797  0.040254  0.186441  0.027542   \n",
       "3   0.019068  0.027542  0.127119  0.095339  0.040254  0.033898  0.023305   \n",
       "4   0.025424  0.027542  0.048729  0.095339  0.029661  0.016949  0.023305   \n",
       "..       ...       ...       ...       ...       ...       ...       ...   \n",
       "85  0.014831  0.016949  0.014831  0.014831  0.014831  0.014831  0.014831   \n",
       "86  0.014831  0.014831  0.014831  0.014831  0.014831  0.014831  0.014831   \n",
       "87  0.014831  0.014831  0.014831  0.014831  0.014831  0.014831  0.014831   \n",
       "88  0.014831  0.014831  0.014831  0.014831  0.014831  0.014831  0.014831   \n",
       "89  0.014831  0.014831  0.014831  0.014831  0.014831  0.014831  0.014831   \n",
       "\n",
       "          7         8         9   ...        40        41        42        43  \\\n",
       "0   0.190678  0.262712  0.156780  ...  0.023305  0.250000  0.057203  0.046610   \n",
       "1   0.190678  0.262712  0.156780  ...  0.023305  0.250000  0.057203  0.046610   \n",
       "2   0.150424  0.038136  0.031780  ...  0.012712  0.040254  0.025424  0.052966   \n",
       "3   0.038136  0.038136  0.033898  ...  0.048729  0.033898  0.072034  0.038136   \n",
       "4   0.059322  0.036017  0.025424  ...  0.046610  0.033898  0.091102  0.021186   \n",
       "..       ...       ...       ...  ...       ...       ...       ...       ...   \n",
       "85  0.014831  0.014831  0.014831  ...  0.014831  0.021186  0.016949  0.014831   \n",
       "86  0.014831  0.014831  0.014831  ...  0.014831  0.021186  0.016949  0.014831   \n",
       "87  0.014831  0.014831  0.014831  ...  0.014831  0.021186  0.016949  0.014831   \n",
       "88  0.014831  0.014831  0.014831  ...  0.014831  0.014831  0.014831  0.014831   \n",
       "89  0.014831  0.016949  0.014831  ...  0.014831  0.014831  0.014831  0.014831   \n",
       "\n",
       "          44        45        46        47        48        49  \n",
       "0   0.161017  0.273305  0.173729  0.069915  0.135593  0.197034  \n",
       "1   0.161017  0.273305  0.173729  0.069915  0.135593  0.190678  \n",
       "2   0.116525  0.029661  0.042373  0.052966  0.078390  0.038136  \n",
       "3   0.016949  0.014831  0.019068  0.082627  0.082627  0.036017  \n",
       "4   0.019068  0.019068  0.019068  0.019068  0.031780  0.042373  \n",
       "..       ...       ...       ...       ...       ...       ...  \n",
       "85  0.014831  0.014831  0.014831  0.019068  0.014831  0.014831  \n",
       "86  0.014831  0.014831  0.014831  0.019068  0.014831  0.014831  \n",
       "87  0.014831  0.014831  0.014831  0.019068  0.014831  0.014831  \n",
       "88  0.014831  0.014831  0.016949  0.014831  0.014831  0.014831  \n",
       "89  0.014831  0.014831  0.016949  0.021186  0.014831  0.014831  \n",
       "\n",
       "[90 rows x 50 columns]"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "passive_error_list = list()\n",
    "for i in range(50):\n",
    "    print(i+1,' time')\n",
    "    error = passive_learning(train_x, train_y, test_x, test_y)\n",
    "    passive_error_list.append(error)\n",
    "passive_error_list = pd.DataFrame(passive_error_list).T\n",
    "passive_error_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-20T06:58:59.388963Z",
     "start_time": "2019-11-20T06:58:59.349605Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>80</th>\n",
       "      <th>81</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "      <th>85</th>\n",
       "      <th>86</th>\n",
       "      <th>87</th>\n",
       "      <th>88</th>\n",
       "      <th>89</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.228814</td>\n",
       "      <td>0.228814</td>\n",
       "      <td>0.114407</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.091102</td>\n",
       "      <td>0.091102</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.046610</td>\n",
       "      <td>0.036017</td>\n",
       "      <td>0.040254</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.150424</td>\n",
       "      <td>0.150424</td>\n",
       "      <td>0.171610</td>\n",
       "      <td>0.127119</td>\n",
       "      <td>0.048729</td>\n",
       "      <td>0.042373</td>\n",
       "      <td>0.038136</td>\n",
       "      <td>0.057203</td>\n",
       "      <td>0.046610</td>\n",
       "      <td>0.046610</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.173729</td>\n",
       "      <td>0.137712</td>\n",
       "      <td>0.067797</td>\n",
       "      <td>0.095339</td>\n",
       "      <td>0.095339</td>\n",
       "      <td>0.095339</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.158898</td>\n",
       "      <td>0.158898</td>\n",
       "      <td>0.040254</td>\n",
       "      <td>0.040254</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.222458</td>\n",
       "      <td>0.222458</td>\n",
       "      <td>0.186441</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.190678</td>\n",
       "      <td>0.190678</td>\n",
       "      <td>0.150424</td>\n",
       "      <td>0.038136</td>\n",
       "      <td>0.059322</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.262712</td>\n",
       "      <td>0.262712</td>\n",
       "      <td>0.038136</td>\n",
       "      <td>0.038136</td>\n",
       "      <td>0.036017</td>\n",
       "      <td>0.036017</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.044492</td>\n",
       "      <td>0.040254</td>\n",
       "      <td>0.040254</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.156780</td>\n",
       "      <td>0.156780</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.042373</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.048729</td>\n",
       "      <td>0.048729</td>\n",
       "      <td>0.063559</td>\n",
       "      <td>0.065678</td>\n",
       "      <td>0.080508</td>\n",
       "      <td>0.038136</td>\n",
       "      <td>0.042373</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.069915</td>\n",
       "      <td>0.069915</td>\n",
       "      <td>0.040254</td>\n",
       "      <td>0.038136</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.042373</td>\n",
       "      <td>0.042373</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.067797</td>\n",
       "      <td>0.067797</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.067797</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.173729</td>\n",
       "      <td>0.173729</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.057203</td>\n",
       "      <td>0.057203</td>\n",
       "      <td>0.040254</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.042373</td>\n",
       "      <td>0.042373</td>\n",
       "      <td>0.042373</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.097458</td>\n",
       "      <td>0.156780</td>\n",
       "      <td>0.173729</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.133475</td>\n",
       "      <td>0.133475</td>\n",
       "      <td>0.141949</td>\n",
       "      <td>0.052966</td>\n",
       "      <td>0.061441</td>\n",
       "      <td>0.038136</td>\n",
       "      <td>0.038136</td>\n",
       "      <td>0.038136</td>\n",
       "      <td>0.038136</td>\n",
       "      <td>0.042373</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.055085</td>\n",
       "      <td>0.059322</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.036017</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.072034</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.038136</td>\n",
       "      <td>0.038136</td>\n",
       "      <td>0.038136</td>\n",
       "      <td>0.036017</td>\n",
       "      <td>0.038136</td>\n",
       "      <td>0.038136</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.099576</td>\n",
       "      <td>0.099576</td>\n",
       "      <td>0.154661</td>\n",
       "      <td>0.173729</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.097458</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.129237</td>\n",
       "      <td>0.067797</td>\n",
       "      <td>0.093220</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.036017</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.173729</td>\n",
       "      <td>0.173729</td>\n",
       "      <td>0.141949</td>\n",
       "      <td>0.038136</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.133475</td>\n",
       "      <td>0.133475</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.040254</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.173729</td>\n",
       "      <td>0.173729</td>\n",
       "      <td>0.046610</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.042373</td>\n",
       "      <td>0.059322</td>\n",
       "      <td>0.042373</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.048729</td>\n",
       "      <td>0.048729</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.063559</td>\n",
       "      <td>0.063559</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.061441</td>\n",
       "      <td>0.055085</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.152542</td>\n",
       "      <td>0.152542</td>\n",
       "      <td>0.076271</td>\n",
       "      <td>0.063559</td>\n",
       "      <td>0.080508</td>\n",
       "      <td>0.055085</td>\n",
       "      <td>0.057203</td>\n",
       "      <td>0.048729</td>\n",
       "      <td>0.048729</td>\n",
       "      <td>0.040254</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.016949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.097458</td>\n",
       "      <td>0.097458</td>\n",
       "      <td>0.067797</td>\n",
       "      <td>0.044492</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.120763</td>\n",
       "      <td>0.148305</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.173729</td>\n",
       "      <td>0.173729</td>\n",
       "      <td>0.091102</td>\n",
       "      <td>0.112288</td>\n",
       "      <td>0.065678</td>\n",
       "      <td>0.048729</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.065678</td>\n",
       "      <td>0.072034</td>\n",
       "      <td>0.061441</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.139831</td>\n",
       "      <td>0.139831</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.038136</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.046610</td>\n",
       "      <td>0.046610</td>\n",
       "      <td>0.038136</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.038136</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.173729</td>\n",
       "      <td>0.173729</td>\n",
       "      <td>0.141949</td>\n",
       "      <td>0.173729</td>\n",
       "      <td>0.057203</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.038136</td>\n",
       "      <td>0.038136</td>\n",
       "      <td>0.038136</td>\n",
       "      <td>0.038136</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.173729</td>\n",
       "      <td>0.173729</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.137712</td>\n",
       "      <td>0.137712</td>\n",
       "      <td>0.161017</td>\n",
       "      <td>0.040254</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.084746</td>\n",
       "      <td>0.084746</td>\n",
       "      <td>0.065678</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.040254</td>\n",
       "      <td>0.074153</td>\n",
       "      <td>0.057203</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.055085</td>\n",
       "      <td>0.059322</td>\n",
       "      <td>0.057203</td>\n",
       "      <td>0.048729</td>\n",
       "      <td>0.042373</td>\n",
       "      <td>0.042373</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.048729</td>\n",
       "      <td>0.046610</td>\n",
       "      <td>0.048729</td>\n",
       "      <td>0.038136</td>\n",
       "      <td>0.038136</td>\n",
       "      <td>0.040254</td>\n",
       "      <td>0.038136</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.040254</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.038136</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.057203</td>\n",
       "      <td>0.057203</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.072034</td>\n",
       "      <td>0.091102</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.036017</td>\n",
       "      <td>0.036017</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.046610</td>\n",
       "      <td>0.046610</td>\n",
       "      <td>0.052966</td>\n",
       "      <td>0.038136</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.161017</td>\n",
       "      <td>0.161017</td>\n",
       "      <td>0.116525</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.273305</td>\n",
       "      <td>0.273305</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.173729</td>\n",
       "      <td>0.173729</td>\n",
       "      <td>0.042373</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.069915</td>\n",
       "      <td>0.069915</td>\n",
       "      <td>0.052966</td>\n",
       "      <td>0.082627</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.080508</td>\n",
       "      <td>0.040254</td>\n",
       "      <td>0.042373</td>\n",
       "      <td>0.036017</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.021186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.135593</td>\n",
       "      <td>0.135593</td>\n",
       "      <td>0.078390</td>\n",
       "      <td>0.082627</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.197034</td>\n",
       "      <td>0.190678</td>\n",
       "      <td>0.038136</td>\n",
       "      <td>0.036017</td>\n",
       "      <td>0.042373</td>\n",
       "      <td>0.038136</td>\n",
       "      <td>0.042373</td>\n",
       "      <td>0.036017</td>\n",
       "      <td>0.036017</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50 rows × 90 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6   \\\n",
       "0   0.228814  0.228814  0.114407  0.019068  0.025424  0.029661  0.033898   \n",
       "1   0.091102  0.091102  0.033898  0.027542  0.027542  0.046610  0.036017   \n",
       "2   0.150424  0.150424  0.171610  0.127119  0.048729  0.042373  0.038136   \n",
       "3   0.173729  0.137712  0.067797  0.095339  0.095339  0.095339  0.033898   \n",
       "4   0.158898  0.158898  0.040254  0.040254  0.029661  0.016949  0.019068   \n",
       "5   0.222458  0.222458  0.186441  0.033898  0.016949  0.027542  0.014831   \n",
       "6   0.023305  0.023305  0.027542  0.023305  0.023305  0.023305  0.025424   \n",
       "7   0.190678  0.190678  0.150424  0.038136  0.059322  0.019068  0.033898   \n",
       "8   0.262712  0.262712  0.038136  0.038136  0.036017  0.036017  0.019068   \n",
       "9   0.156780  0.156780  0.031780  0.033898  0.025424  0.042373  0.023305   \n",
       "10  0.048729  0.048729  0.063559  0.065678  0.080508  0.038136  0.042373   \n",
       "11  0.069915  0.069915  0.040254  0.038136  0.019068  0.027542  0.027542   \n",
       "12  0.042373  0.042373  0.031780  0.021186  0.027542  0.027542  0.027542   \n",
       "13  0.067797  0.067797  0.033898  0.067797  0.019068  0.033898  0.027542   \n",
       "14  0.173729  0.173729  0.031780  0.023305  0.027542  0.027542  0.021186   \n",
       "15  0.057203  0.057203  0.040254  0.025424  0.025424  0.027542  0.031780   \n",
       "16  0.042373  0.042373  0.042373  0.031780  0.019068  0.025424  0.021186   \n",
       "17  0.097458  0.156780  0.173729  0.029661  0.029661  0.031780  0.027542   \n",
       "18  0.133475  0.133475  0.141949  0.052966  0.061441  0.038136  0.038136   \n",
       "19  0.055085  0.059322  0.019068  0.036017  0.029661  0.019068  0.016949   \n",
       "20  0.375000  0.072034  0.031780  0.038136  0.038136  0.038136  0.036017   \n",
       "21  0.099576  0.099576  0.154661  0.173729  0.125000  0.097458  0.014831   \n",
       "22  0.125000  0.129237  0.067797  0.093220  0.016949  0.036017  0.025424   \n",
       "23  0.173729  0.173729  0.141949  0.038136  0.029661  0.029661  0.027542   \n",
       "24  0.133475  0.133475  0.033898  0.040254  0.033898  0.033898  0.029661   \n",
       "25  0.375000  0.375000  0.031780  0.012712  0.010593  0.012712  0.012712   \n",
       "26  0.173729  0.173729  0.046610  0.021186  0.019068  0.025424  0.027542   \n",
       "27  0.042373  0.059322  0.042373  0.023305  0.019068  0.021186  0.023305   \n",
       "28  0.048729  0.048729  0.016949  0.063559  0.063559  0.019068  0.061441   \n",
       "29  0.152542  0.152542  0.076271  0.063559  0.080508  0.055085  0.057203   \n",
       "30  0.097458  0.097458  0.067797  0.044492  0.023305  0.021186  0.021186   \n",
       "31  0.120763  0.148305  0.019068  0.021186  0.019068  0.021186  0.021186   \n",
       "32  0.173729  0.173729  0.091102  0.112288  0.065678  0.048729  0.033898   \n",
       "33  0.139831  0.139831  0.031780  0.031780  0.019068  0.038136  0.033898   \n",
       "34  0.046610  0.046610  0.038136  0.033898  0.038136  0.029661  0.023305   \n",
       "35  0.173729  0.173729  0.141949  0.173729  0.057203  0.033898  0.038136   \n",
       "36  0.173729  0.173729  0.029661  0.027542  0.031780  0.019068  0.019068   \n",
       "37  0.137712  0.137712  0.161017  0.040254  0.033898  0.029661  0.027542   \n",
       "38  0.084746  0.084746  0.065678  0.019068  0.016949  0.023305  0.025424   \n",
       "39  0.040254  0.074153  0.057203  0.031780  0.055085  0.059322  0.057203   \n",
       "40  0.023305  0.023305  0.012712  0.048729  0.046610  0.048729  0.038136   \n",
       "41  0.250000  0.250000  0.040254  0.033898  0.033898  0.038136  0.027542   \n",
       "42  0.057203  0.057203  0.025424  0.072034  0.091102  0.025424  0.025424   \n",
       "43  0.046610  0.046610  0.052966  0.038136  0.021186  0.019068  0.023305   \n",
       "44  0.161017  0.161017  0.116525  0.016949  0.019068  0.014831  0.016949   \n",
       "45  0.273305  0.273305  0.029661  0.014831  0.019068  0.023305  0.031780   \n",
       "46  0.173729  0.173729  0.042373  0.019068  0.019068  0.016949  0.021186   \n",
       "47  0.069915  0.069915  0.052966  0.082627  0.019068  0.080508  0.040254   \n",
       "48  0.135593  0.135593  0.078390  0.082627  0.031780  0.027542  0.023305   \n",
       "49  0.197034  0.190678  0.038136  0.036017  0.042373  0.038136  0.042373   \n",
       "\n",
       "          7         8         9   ...        80        81        82        83  \\\n",
       "0   0.033898  0.027542  0.021186  ...  0.014831  0.019068  0.019068  0.014831   \n",
       "1   0.040254  0.019068  0.031780  ...  0.014831  0.014831  0.014831  0.014831   \n",
       "2   0.057203  0.046610  0.046610  ...  0.014831  0.014831  0.014831  0.014831   \n",
       "3   0.021186  0.033898  0.027542  ...  0.014831  0.014831  0.014831  0.014831   \n",
       "4   0.033898  0.023305  0.023305  ...  0.014831  0.014831  0.014831  0.014831   \n",
       "5   0.016949  0.014831  0.023305  ...  0.014831  0.014831  0.014831  0.014831   \n",
       "6   0.023305  0.023305  0.019068  ...  0.014831  0.014831  0.014831  0.014831   \n",
       "7   0.033898  0.029661  0.033898  ...  0.014831  0.014831  0.014831  0.014831   \n",
       "8   0.044492  0.040254  0.040254  ...  0.014831  0.014831  0.014831  0.014831   \n",
       "9   0.025424  0.025424  0.021186  ...  0.014831  0.014831  0.014831  0.014831   \n",
       "10  0.019068  0.016949  0.012712  ...  0.014831  0.014831  0.014831  0.014831   \n",
       "11  0.027542  0.031780  0.021186  ...  0.021186  0.014831  0.014831  0.014831   \n",
       "12  0.021186  0.031780  0.025424  ...  0.014831  0.014831  0.014831  0.014831   \n",
       "13  0.025424  0.027542  0.016949  ...  0.014831  0.014831  0.016949  0.016949   \n",
       "14  0.029661  0.033898  0.033898  ...  0.016949  0.016949  0.016949  0.016949   \n",
       "15  0.033898  0.033898  0.033898  ...  0.014831  0.014831  0.014831  0.014831   \n",
       "16  0.023305  0.012712  0.019068  ...  0.014831  0.014831  0.014831  0.014831   \n",
       "17  0.014831  0.023305  0.021186  ...  0.016949  0.016949  0.016949  0.016949   \n",
       "18  0.038136  0.038136  0.042373  ...  0.014831  0.014831  0.014831  0.014831   \n",
       "19  0.027542  0.027542  0.027542  ...  0.014831  0.014831  0.014831  0.014831   \n",
       "20  0.038136  0.038136  0.027542  ...  0.014831  0.014831  0.014831  0.014831   \n",
       "21  0.021186  0.014831  0.014831  ...  0.012712  0.014831  0.014831  0.014831   \n",
       "22  0.025424  0.021186  0.033898  ...  0.014831  0.014831  0.016949  0.014831   \n",
       "23  0.027542  0.027542  0.023305  ...  0.016949  0.016949  0.016949  0.016949   \n",
       "24  0.033898  0.023305  0.023305  ...  0.021186  0.021186  0.014831  0.016949   \n",
       "25  0.019068  0.019068  0.016949  ...  0.021186  0.016949  0.016949  0.016949   \n",
       "26  0.031780  0.031780  0.027542  ...  0.014831  0.014831  0.014831  0.016949   \n",
       "27  0.027542  0.023305  0.027542  ...  0.014831  0.014831  0.014831  0.014831   \n",
       "28  0.055085  0.016949  0.014831  ...  0.016949  0.014831  0.014831  0.014831   \n",
       "29  0.048729  0.048729  0.040254  ...  0.016949  0.016949  0.021186  0.021186   \n",
       "30  0.014831  0.016949  0.016949  ...  0.014831  0.014831  0.014831  0.014831   \n",
       "31  0.021186  0.021186  0.019068  ...  0.014831  0.014831  0.014831  0.014831   \n",
       "32  0.065678  0.072034  0.061441  ...  0.014831  0.014831  0.014831  0.014831   \n",
       "33  0.029661  0.029661  0.029661  ...  0.016949  0.016949  0.016949  0.016949   \n",
       "34  0.019068  0.012712  0.012712  ...  0.014831  0.014831  0.014831  0.014831   \n",
       "35  0.038136  0.038136  0.038136  ...  0.014831  0.014831  0.014831  0.016949   \n",
       "36  0.029661  0.029661  0.027542  ...  0.014831  0.014831  0.014831  0.016949   \n",
       "37  0.031780  0.031780  0.025424  ...  0.014831  0.014831  0.014831  0.014831   \n",
       "38  0.019068  0.021186  0.025424  ...  0.016949  0.014831  0.014831  0.014831   \n",
       "39  0.048729  0.042373  0.042373  ...  0.016949  0.016949  0.016949  0.014831   \n",
       "40  0.038136  0.040254  0.038136  ...  0.014831  0.014831  0.014831  0.014831   \n",
       "41  0.023305  0.029661  0.025424  ...  0.021186  0.021186  0.021186  0.019068   \n",
       "42  0.025424  0.036017  0.036017  ...  0.019068  0.021186  0.021186  0.021186   \n",
       "43  0.023305  0.025424  0.031780  ...  0.014831  0.014831  0.014831  0.014831   \n",
       "44  0.019068  0.019068  0.014831  ...  0.014831  0.014831  0.014831  0.014831   \n",
       "45  0.019068  0.019068  0.027542  ...  0.021186  0.021186  0.021186  0.014831   \n",
       "46  0.019068  0.019068  0.019068  ...  0.014831  0.014831  0.014831  0.014831   \n",
       "47  0.042373  0.036017  0.019068  ...  0.014831  0.014831  0.016949  0.016949   \n",
       "48  0.027542  0.031780  0.023305  ...  0.014831  0.014831  0.014831  0.014831   \n",
       "49  0.036017  0.036017  0.027542  ...  0.016949  0.016949  0.016949  0.016949   \n",
       "\n",
       "          84        85        86        87        88        89  \n",
       "0   0.014831  0.014831  0.014831  0.014831  0.014831  0.014831  \n",
       "1   0.014831  0.016949  0.014831  0.014831  0.014831  0.014831  \n",
       "2   0.014831  0.014831  0.014831  0.014831  0.014831  0.014831  \n",
       "3   0.014831  0.014831  0.014831  0.014831  0.014831  0.014831  \n",
       "4   0.014831  0.014831  0.014831  0.014831  0.014831  0.014831  \n",
       "5   0.014831  0.014831  0.014831  0.014831  0.014831  0.014831  \n",
       "6   0.014831  0.014831  0.014831  0.014831  0.014831  0.014831  \n",
       "7   0.014831  0.014831  0.014831  0.014831  0.014831  0.014831  \n",
       "8   0.014831  0.014831  0.014831  0.014831  0.014831  0.016949  \n",
       "9   0.014831  0.014831  0.014831  0.014831  0.014831  0.014831  \n",
       "10  0.014831  0.014831  0.014831  0.014831  0.014831  0.014831  \n",
       "11  0.014831  0.016949  0.016949  0.014831  0.014831  0.016949  \n",
       "12  0.014831  0.014831  0.014831  0.014831  0.014831  0.014831  \n",
       "13  0.016949  0.014831  0.014831  0.014831  0.014831  0.014831  \n",
       "14  0.016949  0.016949  0.016949  0.014831  0.016949  0.016949  \n",
       "15  0.014831  0.014831  0.014831  0.016949  0.014831  0.014831  \n",
       "16  0.014831  0.014831  0.014831  0.014831  0.014831  0.014831  \n",
       "17  0.014831  0.014831  0.014831  0.014831  0.014831  0.014831  \n",
       "18  0.014831  0.014831  0.014831  0.014831  0.014831  0.014831  \n",
       "19  0.014831  0.014831  0.014831  0.014831  0.014831  0.014831  \n",
       "20  0.014831  0.014831  0.014831  0.014831  0.014831  0.014831  \n",
       "21  0.014831  0.014831  0.014831  0.014831  0.014831  0.014831  \n",
       "22  0.014831  0.014831  0.014831  0.014831  0.014831  0.014831  \n",
       "23  0.016949  0.016949  0.016949  0.014831  0.014831  0.016949  \n",
       "24  0.016949  0.016949  0.016949  0.014831  0.016949  0.016949  \n",
       "25  0.014831  0.014831  0.014831  0.014831  0.014831  0.014831  \n",
       "26  0.016949  0.016949  0.016949  0.016949  0.014831  0.014831  \n",
       "27  0.012712  0.014831  0.014831  0.014831  0.014831  0.014831  \n",
       "28  0.014831  0.014831  0.014831  0.014831  0.014831  0.014831  \n",
       "29  0.021186  0.021186  0.021186  0.021186  0.021186  0.016949  \n",
       "30  0.014831  0.016949  0.014831  0.016949  0.016949  0.014831  \n",
       "31  0.014831  0.014831  0.016949  0.014831  0.014831  0.014831  \n",
       "32  0.014831  0.014831  0.014831  0.014831  0.014831  0.014831  \n",
       "33  0.016949  0.016949  0.016949  0.016949  0.016949  0.016949  \n",
       "34  0.014831  0.019068  0.019068  0.019068  0.016949  0.016949  \n",
       "35  0.016949  0.016949  0.016949  0.016949  0.016949  0.014831  \n",
       "36  0.016949  0.014831  0.014831  0.014831  0.014831  0.014831  \n",
       "37  0.014831  0.014831  0.014831  0.014831  0.014831  0.014831  \n",
       "38  0.014831  0.016949  0.019068  0.014831  0.014831  0.016949  \n",
       "39  0.016949  0.016949  0.016949  0.014831  0.014831  0.014831  \n",
       "40  0.014831  0.014831  0.014831  0.014831  0.014831  0.014831  \n",
       "41  0.021186  0.021186  0.021186  0.021186  0.014831  0.014831  \n",
       "42  0.014831  0.016949  0.016949  0.016949  0.014831  0.014831  \n",
       "43  0.014831  0.014831  0.014831  0.014831  0.014831  0.014831  \n",
       "44  0.014831  0.014831  0.014831  0.014831  0.014831  0.014831  \n",
       "45  0.014831  0.014831  0.014831  0.014831  0.014831  0.014831  \n",
       "46  0.014831  0.014831  0.014831  0.014831  0.016949  0.016949  \n",
       "47  0.014831  0.019068  0.019068  0.019068  0.014831  0.021186  \n",
       "48  0.014831  0.014831  0.014831  0.014831  0.014831  0.014831  \n",
       "49  0.014831  0.014831  0.014831  0.014831  0.014831  0.014831  \n",
       "\n",
       "[50 rows x 90 columns]"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "passive_error_list = pd.DataFrame(passive_error_list).T\n",
    "passive_error_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ii. Train a SVM with a pool of 10 randomly selected data points from the training set3 using linear kernel and L1 penalty. Select the parameters of the SVM with 10-fold cross validation. Choose the 10 closest data points in the training set to the hyperplane of the SVM4 and add them to the pool. Do not replace the samples back into the training set. Train a new SVM using the pool. Repeat this process until all training data is used. You will have 90 SVMs that were trained using 10, 20, 30,..., 900 data points and their 90 test errors. You have implemented active learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-20T07:01:05.824863Z",
     "start_time": "2019-11-20T07:01:05.812789Z"
    }
   },
   "outputs": [],
   "source": [
    "def active_learning(train_x, train_y, test_x, test_y):\n",
    "    x_select = pd.DataFrame()\n",
    "    y_select = pd.DataFrame()\n",
    "    # make sure two kind of classes are included\n",
    "    while len(y_select) < 1 or len(np.bincount(y_select)) < 2 or np.bincount(y_select).min() < 2:\n",
    "        # random select 10\n",
    "        random_indexes = np.random.randint(0, len(train_x), 10)\n",
    "        x_select = train_x.iloc[random_indexes]\n",
    "        y_select = train_y.iloc[random_indexes]\n",
    "    # the remian x\n",
    "    remain_x_data = train_x.drop(train_x.index[random_indexes])\n",
    "    remain_y_data = train_y.drop(train_x.index[random_indexes])\n",
    "    \n",
    "    # update index\n",
    "    x_select = x_select.reset_index(drop=True)\n",
    "    y_select = y_select.reset_index(drop=True)\n",
    "    remain_x_data = remain_x_data.reset_index(drop=True)\n",
    "    remain_y_data = remain_y_data.reset_index(drop=True)\n",
    "    \n",
    "    errors = list()\n",
    "    for i in range(90):\n",
    "#         if i < 2:\n",
    "#             CV = 5\n",
    "#         else:\n",
    "#             CV = 10\n",
    "        \n",
    "        parameters = {'C':np.power(10, np.arange(-3, 7,0.5))}\n",
    "        svc = LinearSVC(penalty='l1', dual=False)\n",
    "        clf = GridSearchCV(svc, parameters, cv=5)\n",
    "        clf.fit(x_select, y_select)\n",
    "        best_C = clf.best_params_['C']\n",
    "        \n",
    "        # Refit the model with the best params\n",
    "        svc = LinearSVC(penalty='l1', dual=False, C=best_C)\n",
    "        svc.fit(x_select, y_select)\n",
    "        errors.append(1-svc.score(test_x, test_y))\n",
    "        \n",
    "        if i != 0:\n",
    "            distances = svc.decision_function(remain_x_data)\n",
    "            closest_indexes = np.argsort(abs(distances))[:10]\n",
    "            \n",
    "            # append data to train set\n",
    "            x_select = pd.concat([x_select, remain_x_data.iloc[closest_indexes]],axis=0, ignore_index=True)\n",
    "            y_select = pd.concat([y_select, remain_y_data.iloc[closest_indexes]],axis=0, ignore_index=True)\n",
    "            # delete from remain data\n",
    "            remain_x_data = remain_x_data.drop(remain_x_data.index[closest_indexes])\n",
    "            remain_y_data = remain_y_data.drop(remain_y_data.index[closest_indexes])\n",
    "\n",
    "            #reset index\n",
    "            remain_x_data = remain_x_data.reset_index(drop=True)\n",
    "            remain_y_data = remain_y_data.reset_index(drop=True)\n",
    "    return errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-20T07:24:45.245472Z",
     "start_time": "2019-11-20T07:01:07.605914Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1  time\n",
      "2  time\n",
      "3  time\n",
      "4  time\n",
      "5  time\n",
      "6  time\n",
      "7  time\n",
      "8  time\n",
      "9  time\n",
      "10  time\n",
      "11  time\n",
      "12  time\n",
      "13  time\n",
      "14  time\n",
      "15  time\n",
      "16  time\n",
      "17  time\n",
      "18  time\n",
      "19  time\n",
      "20  time\n",
      "21  time\n",
      "22  time\n",
      "23  time\n",
      "24  time\n",
      "25  time\n",
      "26  time\n",
      "27  time\n",
      "28  time\n",
      "29  time\n",
      "30  time\n",
      "31  time\n",
      "32  time\n",
      "33  time\n",
      "34  time\n",
      "35  time\n",
      "36  time\n",
      "37  time\n",
      "38  time\n",
      "39  time\n",
      "40  time\n",
      "41  time\n",
      "42  time\n",
      "43  time\n",
      "44  time\n",
      "45  time\n",
      "46  time\n",
      "47  time\n",
      "48  time\n",
      "49  time\n",
      "50  time\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.173729</td>\n",
       "      <td>0.220339</td>\n",
       "      <td>0.097458</td>\n",
       "      <td>0.108051</td>\n",
       "      <td>0.059322</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.175847</td>\n",
       "      <td>0.173729</td>\n",
       "      <td>0.042373</td>\n",
       "      <td>0.148305</td>\n",
       "      <td>...</td>\n",
       "      <td>0.154661</td>\n",
       "      <td>0.040254</td>\n",
       "      <td>0.120763</td>\n",
       "      <td>0.095339</td>\n",
       "      <td>0.173729</td>\n",
       "      <td>0.139831</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.188559</td>\n",
       "      <td>0.197034</td>\n",
       "      <td>0.184322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.173729</td>\n",
       "      <td>0.220339</td>\n",
       "      <td>0.050847</td>\n",
       "      <td>0.061441</td>\n",
       "      <td>0.059322</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.175847</td>\n",
       "      <td>0.173729</td>\n",
       "      <td>0.042373</td>\n",
       "      <td>0.148305</td>\n",
       "      <td>...</td>\n",
       "      <td>0.158898</td>\n",
       "      <td>0.055085</td>\n",
       "      <td>0.120763</td>\n",
       "      <td>0.137712</td>\n",
       "      <td>0.173729</td>\n",
       "      <td>0.139831</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.188559</td>\n",
       "      <td>0.213983</td>\n",
       "      <td>0.184322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.072034</td>\n",
       "      <td>0.036017</td>\n",
       "      <td>0.112288</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.038136</td>\n",
       "      <td>0.120763</td>\n",
       "      <td>0.036017</td>\n",
       "      <td>...</td>\n",
       "      <td>0.122881</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.059322</td>\n",
       "      <td>0.133475</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.309322</td>\n",
       "      <td>0.146186</td>\n",
       "      <td>0.139831</td>\n",
       "      <td>0.088983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.040254</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.069915</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.038136</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.046610</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038136</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.135593</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.031780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.199153</td>\n",
       "      <td>0.055085</td>\n",
       "      <td>0.036017</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>...</td>\n",
       "      <td>0.059322</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.025424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6   \\\n",
       "0   0.173729  0.220339  0.097458  0.108051  0.059322  0.375000  0.175847   \n",
       "1   0.173729  0.220339  0.050847  0.061441  0.059322  0.375000  0.175847   \n",
       "2   0.031780  0.029661  0.021186  0.072034  0.036017  0.112288  0.023305   \n",
       "3   0.040254  0.021186  0.069915  0.012712  0.025424  0.038136  0.033898   \n",
       "4   0.014831  0.033898  0.023305  0.021186  0.012712  0.199153  0.055085   \n",
       "..       ...       ...       ...       ...       ...       ...       ...   \n",
       "85  0.016949  0.014831  0.014831  0.014831  0.014831  0.014831  0.014831   \n",
       "86  0.014831  0.016949  0.014831  0.016949  0.014831  0.014831  0.014831   \n",
       "87  0.016949  0.016949  0.014831  0.016949  0.014831  0.014831  0.016949   \n",
       "88  0.016949  0.016949  0.014831  0.016949  0.014831  0.014831  0.016949   \n",
       "89  0.016949  0.016949  0.014831  0.014831  0.014831  0.014831  0.016949   \n",
       "\n",
       "          7         8         9   ...        40        41        42        43  \\\n",
       "0   0.173729  0.042373  0.148305  ...  0.154661  0.040254  0.120763  0.095339   \n",
       "1   0.173729  0.042373  0.148305  ...  0.158898  0.055085  0.120763  0.137712   \n",
       "2   0.038136  0.120763  0.036017  ...  0.122881  0.014831  0.059322  0.133475   \n",
       "3   0.016949  0.046610  0.021186  ...  0.038136  0.021186  0.031780  0.023305   \n",
       "4   0.036017  0.029661  0.014831  ...  0.059322  0.016949  0.023305  0.019068   \n",
       "..       ...       ...       ...  ...       ...       ...       ...       ...   \n",
       "85  0.014831  0.016949  0.014831  ...  0.014831  0.014831  0.016949  0.014831   \n",
       "86  0.014831  0.016949  0.014831  ...  0.014831  0.014831  0.016949  0.014831   \n",
       "87  0.016949  0.016949  0.014831  ...  0.014831  0.014831  0.016949  0.014831   \n",
       "88  0.016949  0.016949  0.016949  ...  0.014831  0.014831  0.016949  0.014831   \n",
       "89  0.016949  0.014831  0.016949  ...  0.016949  0.014831  0.016949  0.014831   \n",
       "\n",
       "          44        45        46        47        48        49  \n",
       "0   0.173729  0.139831  0.375000  0.188559  0.197034  0.184322  \n",
       "1   0.173729  0.139831  0.375000  0.188559  0.213983  0.184322  \n",
       "2   0.023305  0.031780  0.309322  0.146186  0.139831  0.088983  \n",
       "3   0.027542  0.016949  0.135593  0.029661  0.023305  0.031780  \n",
       "4   0.027542  0.012712  0.025424  0.025424  0.025424  0.025424  \n",
       "..       ...       ...       ...       ...       ...       ...  \n",
       "85  0.016949  0.014831  0.014831  0.014831  0.016949  0.014831  \n",
       "86  0.016949  0.014831  0.014831  0.014831  0.016949  0.014831  \n",
       "87  0.016949  0.014831  0.014831  0.014831  0.014831  0.014831  \n",
       "88  0.016949  0.014831  0.014831  0.014831  0.016949  0.014831  \n",
       "89  0.016949  0.014831  0.016949  0.014831  0.016949  0.016949  \n",
       "\n",
       "[90 rows x 50 columns]"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "active_error_list = list()\n",
    "for i in range(50):\n",
    "    print(i+1,' time')\n",
    "    error = active_learning(train_x, train_y, test_x, test_y)\n",
    "    active_error_list.append(error)\n",
    "active_error_list = pd.DataFrame(active_error_list)\n",
    "active_error_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-20T07:24:57.916900Z",
     "start_time": "2019-11-20T07:24:57.878120Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>80</th>\n",
       "      <th>81</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "      <th>85</th>\n",
       "      <th>86</th>\n",
       "      <th>87</th>\n",
       "      <th>88</th>\n",
       "      <th>89</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.173729</td>\n",
       "      <td>0.173729</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.040254</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.220339</td>\n",
       "      <td>0.220339</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.097458</td>\n",
       "      <td>0.050847</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.069915</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.108051</td>\n",
       "      <td>0.061441</td>\n",
       "      <td>0.072034</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.059322</td>\n",
       "      <td>0.059322</td>\n",
       "      <td>0.036017</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.112288</td>\n",
       "      <td>0.038136</td>\n",
       "      <td>0.199153</td>\n",
       "      <td>0.042373</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.175847</td>\n",
       "      <td>0.175847</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.055085</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.173729</td>\n",
       "      <td>0.173729</td>\n",
       "      <td>0.038136</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.036017</td>\n",
       "      <td>0.038136</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.042373</td>\n",
       "      <td>0.042373</td>\n",
       "      <td>0.120763</td>\n",
       "      <td>0.046610</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.148305</td>\n",
       "      <td>0.148305</td>\n",
       "      <td>0.036017</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.122881</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.127119</td>\n",
       "      <td>0.127119</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.146186</td>\n",
       "      <td>0.146186</td>\n",
       "      <td>0.036017</td>\n",
       "      <td>0.050847</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.129237</td>\n",
       "      <td>0.129237</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.057203</td>\n",
       "      <td>0.036017</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.156780</td>\n",
       "      <td>0.156780</td>\n",
       "      <td>0.042373</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.154661</td>\n",
       "      <td>0.154661</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.188559</td>\n",
       "      <td>0.188559</td>\n",
       "      <td>0.036017</td>\n",
       "      <td>0.118644</td>\n",
       "      <td>0.055085</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.116525</td>\n",
       "      <td>0.209746</td>\n",
       "      <td>0.036017</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.046610</td>\n",
       "      <td>0.046610</td>\n",
       "      <td>0.040254</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.161017</td>\n",
       "      <td>0.161017</td>\n",
       "      <td>0.048729</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.055085</td>\n",
       "      <td>0.042373</td>\n",
       "      <td>0.065678</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.173729</td>\n",
       "      <td>0.173729</td>\n",
       "      <td>0.173729</td>\n",
       "      <td>0.046610</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.141949</td>\n",
       "      <td>0.055085</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.088983</td>\n",
       "      <td>0.103814</td>\n",
       "      <td>0.036017</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.095339</td>\n",
       "      <td>0.080508</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.173729</td>\n",
       "      <td>0.173729</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.112288</td>\n",
       "      <td>0.118644</td>\n",
       "      <td>0.080508</td>\n",
       "      <td>0.065678</td>\n",
       "      <td>0.038136</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.040254</td>\n",
       "      <td>0.057203</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.141949</td>\n",
       "      <td>0.141949</td>\n",
       "      <td>0.046610</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.173729</td>\n",
       "      <td>0.173729</td>\n",
       "      <td>0.042373</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.129237</td>\n",
       "      <td>0.103814</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.141949</td>\n",
       "      <td>0.141949</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.095339</td>\n",
       "      <td>0.038136</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.069915</td>\n",
       "      <td>0.069915</td>\n",
       "      <td>0.055085</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.141949</td>\n",
       "      <td>0.281780</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.137712</td>\n",
       "      <td>0.069915</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.116525</td>\n",
       "      <td>0.116525</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.218220</td>\n",
       "      <td>0.182203</td>\n",
       "      <td>0.052966</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.044492</td>\n",
       "      <td>0.038136</td>\n",
       "      <td>0.061441</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.154661</td>\n",
       "      <td>0.158898</td>\n",
       "      <td>0.122881</td>\n",
       "      <td>0.038136</td>\n",
       "      <td>0.059322</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.040254</td>\n",
       "      <td>0.055085</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.120763</td>\n",
       "      <td>0.120763</td>\n",
       "      <td>0.059322</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.095339</td>\n",
       "      <td>0.137712</td>\n",
       "      <td>0.133475</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.173729</td>\n",
       "      <td>0.173729</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.139831</td>\n",
       "      <td>0.139831</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.309322</td>\n",
       "      <td>0.135593</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.188559</td>\n",
       "      <td>0.188559</td>\n",
       "      <td>0.146186</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.197034</td>\n",
       "      <td>0.213983</td>\n",
       "      <td>0.139831</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.184322</td>\n",
       "      <td>0.184322</td>\n",
       "      <td>0.088983</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50 rows × 90 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6   \\\n",
       "0   0.173729  0.173729  0.031780  0.040254  0.014831  0.010593  0.010593   \n",
       "1   0.220339  0.220339  0.029661  0.021186  0.033898  0.023305  0.021186   \n",
       "2   0.097458  0.050847  0.021186  0.069915  0.023305  0.014831  0.012712   \n",
       "3   0.108051  0.061441  0.072034  0.012712  0.021186  0.010593  0.012712   \n",
       "4   0.059322  0.059322  0.036017  0.025424  0.012712  0.021186  0.023305   \n",
       "5   0.375000  0.375000  0.112288  0.038136  0.199153  0.042373  0.021186   \n",
       "6   0.175847  0.175847  0.023305  0.033898  0.055085  0.014831  0.012712   \n",
       "7   0.173729  0.173729  0.038136  0.016949  0.036017  0.038136  0.021186   \n",
       "8   0.042373  0.042373  0.120763  0.046610  0.029661  0.023305  0.023305   \n",
       "9   0.148305  0.148305  0.036017  0.021186  0.014831  0.019068  0.014831   \n",
       "10  0.019068  0.019068  0.122881  0.021186  0.016949  0.016949  0.016949   \n",
       "11  0.127119  0.127119  0.027542  0.016949  0.021186  0.010593  0.010593   \n",
       "12  0.146186  0.146186  0.036017  0.050847  0.016949  0.019068  0.014831   \n",
       "13  0.129237  0.129237  0.027542  0.057203  0.036017  0.025424  0.016949   \n",
       "14  0.156780  0.156780  0.042373  0.025424  0.025424  0.014831  0.016949   \n",
       "15  0.154661  0.154661  0.023305  0.029661  0.010593  0.016949  0.016949   \n",
       "16  0.188559  0.188559  0.036017  0.118644  0.055085  0.031780  0.021186   \n",
       "17  0.116525  0.209746  0.036017  0.027542  0.023305  0.014831  0.016949   \n",
       "18  0.046610  0.046610  0.040254  0.014831  0.010593  0.010593  0.021186   \n",
       "19  0.161017  0.161017  0.048729  0.029661  0.021186  0.014831  0.014831   \n",
       "20  0.055085  0.042373  0.065678  0.012712  0.016949  0.021186  0.016949   \n",
       "21  0.173729  0.173729  0.173729  0.046610  0.031780  0.016949  0.021186   \n",
       "22  0.125000  0.141949  0.055085  0.016949  0.021186  0.010593  0.012712   \n",
       "23  0.088983  0.103814  0.036017  0.033898  0.016949  0.023305  0.014831   \n",
       "24  0.095339  0.080508  0.029661  0.023305  0.016949  0.019068  0.016949   \n",
       "25  0.173729  0.173729  0.027542  0.021186  0.021186  0.012712  0.014831   \n",
       "26  0.112288  0.118644  0.080508  0.065678  0.038136  0.025424  0.019068   \n",
       "27  0.375000  0.375000  0.040254  0.057203  0.019068  0.012712  0.016949   \n",
       "28  0.141949  0.141949  0.046610  0.012712  0.029661  0.023305  0.016949   \n",
       "29  0.173729  0.173729  0.042373  0.023305  0.023305  0.021186  0.012712   \n",
       "30  0.129237  0.103814  0.031780  0.021186  0.029661  0.014831  0.014831   \n",
       "31  0.141949  0.141949  0.031780  0.033898  0.021186  0.021186  0.014831   \n",
       "32  0.375000  0.375000  0.095339  0.038136  0.033898  0.012712  0.012712   \n",
       "33  0.069915  0.069915  0.055085  0.029661  0.019068  0.016949  0.016949   \n",
       "34  0.141949  0.281780  0.029661  0.021186  0.019068  0.021186  0.019068   \n",
       "35  0.375000  0.375000  0.137712  0.069915  0.029661  0.025424  0.012712   \n",
       "36  0.116525  0.116525  0.023305  0.023305  0.019068  0.012712  0.010593   \n",
       "37  0.218220  0.182203  0.052966  0.027542  0.010593  0.019068  0.021186   \n",
       "38  0.029661  0.029661  0.021186  0.014831  0.014831  0.016949  0.016949   \n",
       "39  0.044492  0.038136  0.061441  0.019068  0.016949  0.021186  0.014831   \n",
       "40  0.154661  0.158898  0.122881  0.038136  0.059322  0.025424  0.021186   \n",
       "41  0.040254  0.055085  0.014831  0.021186  0.016949  0.012712  0.016949   \n",
       "42  0.120763  0.120763  0.059322  0.031780  0.023305  0.014831  0.016949   \n",
       "43  0.095339  0.137712  0.133475  0.023305  0.019068  0.016949  0.019068   \n",
       "44  0.173729  0.173729  0.023305  0.027542  0.027542  0.016949  0.016949   \n",
       "45  0.139831  0.139831  0.031780  0.016949  0.012712  0.021186  0.014831   \n",
       "46  0.375000  0.375000  0.309322  0.135593  0.025424  0.021186  0.016949   \n",
       "47  0.188559  0.188559  0.146186  0.029661  0.025424  0.016949  0.027542   \n",
       "48  0.197034  0.213983  0.139831  0.023305  0.025424  0.019068  0.016949   \n",
       "49  0.184322  0.184322  0.088983  0.031780  0.025424  0.023305  0.021186   \n",
       "\n",
       "          7         8         9   ...        80        81        82        83  \\\n",
       "0   0.019068  0.014831  0.014831  ...  0.016949  0.016949  0.016949  0.016949   \n",
       "1   0.014831  0.014831  0.019068  ...  0.016949  0.016949  0.016949  0.016949   \n",
       "2   0.016949  0.014831  0.016949  ...  0.016949  0.016949  0.016949  0.016949   \n",
       "3   0.021186  0.019068  0.016949  ...  0.016949  0.016949  0.016949  0.016949   \n",
       "4   0.016949  0.016949  0.016949  ...  0.014831  0.014831  0.014831  0.014831   \n",
       "5   0.016949  0.014831  0.016949  ...  0.014831  0.014831  0.014831  0.014831   \n",
       "6   0.016949  0.016949  0.016949  ...  0.014831  0.014831  0.014831  0.014831   \n",
       "7   0.019068  0.014831  0.016949  ...  0.014831  0.014831  0.014831  0.016949   \n",
       "8   0.016949  0.016949  0.016949  ...  0.016949  0.016949  0.016949  0.016949   \n",
       "9   0.016949  0.016949  0.016949  ...  0.014831  0.014831  0.014831  0.014831   \n",
       "10  0.016949  0.016949  0.016949  ...  0.014831  0.014831  0.014831  0.014831   \n",
       "11  0.014831  0.016949  0.014831  ...  0.016949  0.016949  0.016949  0.016949   \n",
       "12  0.016949  0.014831  0.016949  ...  0.016949  0.016949  0.014831  0.014831   \n",
       "13  0.019068  0.019068  0.021186  ...  0.016949  0.016949  0.016949  0.016949   \n",
       "14  0.016949  0.016949  0.016949  ...  0.016949  0.016949  0.016949  0.014831   \n",
       "15  0.016949  0.016949  0.016949  ...  0.014831  0.014831  0.014831  0.014831   \n",
       "16  0.021186  0.014831  0.016949  ...  0.014831  0.016949  0.014831  0.014831   \n",
       "17  0.016949  0.016949  0.016949  ...  0.016949  0.016949  0.016949  0.016949   \n",
       "18  0.019068  0.016949  0.016949  ...  0.014831  0.016949  0.014831  0.014831   \n",
       "19  0.021186  0.014831  0.016949  ...  0.014831  0.016949  0.016949  0.016949   \n",
       "20  0.016949  0.016949  0.014831  ...  0.014831  0.014831  0.016949  0.016949   \n",
       "21  0.016949  0.019068  0.016949  ...  0.014831  0.014831  0.016949  0.016949   \n",
       "22  0.014831  0.021186  0.016949  ...  0.014831  0.014831  0.014831  0.014831   \n",
       "23  0.019068  0.016949  0.016949  ...  0.014831  0.014831  0.014831  0.014831   \n",
       "24  0.016949  0.016949  0.016949  ...  0.014831  0.014831  0.014831  0.016949   \n",
       "25  0.016949  0.016949  0.016949  ...  0.014831  0.014831  0.014831  0.014831   \n",
       "26  0.021186  0.016949  0.014831  ...  0.014831  0.014831  0.014831  0.014831   \n",
       "27  0.016949  0.016949  0.016949  ...  0.014831  0.014831  0.014831  0.014831   \n",
       "28  0.014831  0.016949  0.016949  ...  0.014831  0.014831  0.014831  0.014831   \n",
       "29  0.019068  0.014831  0.014831  ...  0.016949  0.014831  0.014831  0.014831   \n",
       "30  0.014831  0.016949  0.016949  ...  0.016949  0.016949  0.016949  0.014831   \n",
       "31  0.016949  0.014831  0.016949  ...  0.014831  0.014831  0.014831  0.014831   \n",
       "32  0.019068  0.016949  0.016949  ...  0.016949  0.016949  0.014831  0.014831   \n",
       "33  0.016949  0.016949  0.014831  ...  0.016949  0.016949  0.016949  0.014831   \n",
       "34  0.016949  0.016949  0.016949  ...  0.016949  0.016949  0.016949  0.016949   \n",
       "35  0.012712  0.021186  0.016949  ...  0.016949  0.016949  0.016949  0.014831   \n",
       "36  0.012712  0.014831  0.016949  ...  0.014831  0.016949  0.014831  0.014831   \n",
       "37  0.016949  0.016949  0.014831  ...  0.016949  0.016949  0.016949  0.016949   \n",
       "38  0.016949  0.016949  0.016949  ...  0.014831  0.016949  0.014831  0.014831   \n",
       "39  0.019068  0.014831  0.014831  ...  0.014831  0.014831  0.014831  0.016949   \n",
       "40  0.019068  0.014831  0.016949  ...  0.016949  0.016949  0.014831  0.014831   \n",
       "41  0.019068  0.014831  0.016949  ...  0.014831  0.014831  0.014831  0.014831   \n",
       "42  0.016949  0.016949  0.016949  ...  0.014831  0.014831  0.014831  0.016949   \n",
       "43  0.014831  0.014831  0.016949  ...  0.014831  0.014831  0.014831  0.014831   \n",
       "44  0.016949  0.014831  0.016949  ...  0.016949  0.016949  0.016949  0.016949   \n",
       "45  0.016949  0.016949  0.014831  ...  0.014831  0.014831  0.014831  0.014831   \n",
       "46  0.016949  0.019068  0.016949  ...  0.014831  0.016949  0.014831  0.014831   \n",
       "47  0.019068  0.016949  0.016949  ...  0.016949  0.016949  0.016949  0.014831   \n",
       "48  0.019068  0.016949  0.014831  ...  0.016949  0.014831  0.014831  0.014831   \n",
       "49  0.021186  0.014831  0.014831  ...  0.014831  0.016949  0.014831  0.014831   \n",
       "\n",
       "          84        85        86        87        88        89  \n",
       "0   0.016949  0.016949  0.014831  0.016949  0.016949  0.016949  \n",
       "1   0.016949  0.014831  0.016949  0.016949  0.016949  0.016949  \n",
       "2   0.016949  0.014831  0.014831  0.014831  0.014831  0.014831  \n",
       "3   0.016949  0.014831  0.016949  0.016949  0.016949  0.014831  \n",
       "4   0.014831  0.014831  0.014831  0.014831  0.014831  0.014831  \n",
       "5   0.014831  0.014831  0.014831  0.014831  0.014831  0.014831  \n",
       "6   0.014831  0.014831  0.014831  0.016949  0.016949  0.016949  \n",
       "7   0.014831  0.014831  0.014831  0.016949  0.016949  0.016949  \n",
       "8   0.016949  0.016949  0.016949  0.016949  0.016949  0.014831  \n",
       "9   0.014831  0.014831  0.014831  0.014831  0.016949  0.016949  \n",
       "10  0.014831  0.014831  0.014831  0.014831  0.014831  0.014831  \n",
       "11  0.014831  0.014831  0.016949  0.016949  0.016949  0.016949  \n",
       "12  0.014831  0.014831  0.016949  0.016949  0.016949  0.016949  \n",
       "13  0.016949  0.016949  0.016949  0.014831  0.014831  0.014831  \n",
       "14  0.014831  0.014831  0.014831  0.016949  0.016949  0.014831  \n",
       "15  0.014831  0.014831  0.014831  0.014831  0.016949  0.016949  \n",
       "16  0.014831  0.014831  0.014831  0.014831  0.014831  0.014831  \n",
       "17  0.016949  0.016949  0.014831  0.016949  0.016949  0.016949  \n",
       "18  0.014831  0.014831  0.014831  0.014831  0.014831  0.014831  \n",
       "19  0.016949  0.016949  0.016949  0.016949  0.016949  0.014831  \n",
       "20  0.016949  0.016949  0.014831  0.014831  0.014831  0.014831  \n",
       "21  0.014831  0.014831  0.016949  0.016949  0.016949  0.016949  \n",
       "22  0.014831  0.014831  0.014831  0.014831  0.014831  0.014831  \n",
       "23  0.014831  0.014831  0.014831  0.014831  0.014831  0.014831  \n",
       "24  0.016949  0.014831  0.016949  0.016949  0.016949  0.016949  \n",
       "25  0.014831  0.014831  0.014831  0.014831  0.014831  0.016949  \n",
       "26  0.014831  0.014831  0.014831  0.014831  0.016949  0.016949  \n",
       "27  0.014831  0.014831  0.016949  0.016949  0.016949  0.014831  \n",
       "28  0.014831  0.014831  0.014831  0.014831  0.014831  0.014831  \n",
       "29  0.014831  0.014831  0.014831  0.016949  0.016949  0.014831  \n",
       "30  0.014831  0.014831  0.016949  0.016949  0.016949  0.014831  \n",
       "31  0.014831  0.014831  0.014831  0.014831  0.014831  0.014831  \n",
       "32  0.014831  0.014831  0.016949  0.016949  0.016949  0.016949  \n",
       "33  0.014831  0.014831  0.014831  0.016949  0.016949  0.016949  \n",
       "34  0.016949  0.014831  0.014831  0.014831  0.014831  0.014831  \n",
       "35  0.014831  0.014831  0.014831  0.014831  0.014831  0.014831  \n",
       "36  0.014831  0.014831  0.014831  0.014831  0.014831  0.014831  \n",
       "37  0.014831  0.014831  0.014831  0.014831  0.014831  0.014831  \n",
       "38  0.014831  0.014831  0.014831  0.014831  0.014831  0.014831  \n",
       "39  0.016949  0.016949  0.016949  0.016949  0.016949  0.014831  \n",
       "40  0.014831  0.014831  0.014831  0.014831  0.014831  0.016949  \n",
       "41  0.014831  0.014831  0.014831  0.014831  0.014831  0.014831  \n",
       "42  0.016949  0.016949  0.016949  0.016949  0.016949  0.016949  \n",
       "43  0.014831  0.014831  0.014831  0.014831  0.014831  0.014831  \n",
       "44  0.016949  0.016949  0.016949  0.016949  0.016949  0.016949  \n",
       "45  0.014831  0.014831  0.014831  0.014831  0.014831  0.014831  \n",
       "46  0.014831  0.014831  0.014831  0.014831  0.014831  0.016949  \n",
       "47  0.014831  0.014831  0.014831  0.014831  0.014831  0.014831  \n",
       "48  0.014831  0.016949  0.016949  0.014831  0.016949  0.016949  \n",
       "49  0.014831  0.014831  0.014831  0.014831  0.014831  0.016949  \n",
       "\n",
       "[50 rows x 90 columns]"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "active_error_list = pd.DataFrame(active_error_list).T\n",
    "active_error_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c) Average the 50 test errors for each of the incrementally trained 90 SVMs in 2(b)i and 2(b)ii. By doing so, you are performing a Monte Carlo simulation. Plot average test error versus number of training instances for both active and passive learners on the same figure and report your conclusions. Here, you are actually obtaining a learning curve by Monte-Carlo simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-20T07:25:31.247259Z",
     "start_time": "2019-11-20T07:25:31.071564Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot the average result\n",
    "plt.plot(np.arange(1,91)*10, passive_error_list.mean() , c='r',label='Passive Learning')\n",
    "plt.plot(np.arange(1,91)*10, active_error_list.mean(), c='b', label='Active Learning')\n",
    "plt.xlabel('The Number of Training Data')\n",
    "plt.ylabel('Error Rate')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### conclusion: When the size of dataset is small, the passive learning is better than active learning. If the size is large, these two methods have the same performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
